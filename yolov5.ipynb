{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"57eb0eaa318d4b40a07c22d70472c103":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07b3a9ca01b74360b2c7a3d5b47f1fe9","IPY_MODEL_af034548f4664e6a8c4b7530a0f859ea","IPY_MODEL_ed9bb988ff714b308300033581caf93b"],"layout":"IPY_MODEL_de4f8cfa2f1e4f51af2935bd14105322"}},"07b3a9ca01b74360b2c7a3d5b47f1fe9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6eeee67e46644c6c9518043ea5357a28","placeholder":"​","style":"IPY_MODEL_dbd4418aae6641b0bf4c81b225ff9822","value":"100%"}},"af034548f4664e6a8c4b7530a0f859ea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ebec48a2c34535b86182e837289495","max":5103355,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0bbba5c4d4044aa0b9044d7fa30245ed","value":5103355}},"ed9bb988ff714b308300033581caf93b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87ea85d6a1624e04a454d719ccdfcf78","placeholder":"​","style":"IPY_MODEL_30b7b05f25ac45309555054929ba7333","value":" 4.87M/4.87M [00:00&lt;00:00, 26.1MB/s]"}},"de4f8cfa2f1e4f51af2935bd14105322":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6eeee67e46644c6c9518043ea5357a28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbd4418aae6641b0bf4c81b225ff9822":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18ebec48a2c34535b86182e837289495":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bbba5c4d4044aa0b9044d7fa30245ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87ea85d6a1624e04a454d719ccdfcf78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30b7b05f25ac45309555054929ba7333":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f29acf7fe594a81a392c3a43e835c63":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87f0057ff99248b5a571f982cf473976","IPY_MODEL_d4d0ef9764854afda3af4403b073d30a","IPY_MODEL_cf743e501dbc4c9cb182cad6c58b0718"],"layout":"IPY_MODEL_77144cf2dcb746eab5aeee29ae7a61bb"}},"87f0057ff99248b5a571f982cf473976":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3067e8b271b245e19fd66830a4357187","placeholder":"​","style":"IPY_MODEL_447138e23d5448d5a437a59006bf712c","value":"100%"}},"d4d0ef9764854afda3af4403b073d30a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cc135fac91e45debca0cb9f323296a2","max":11034043,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b747ffafa25a4636b3c00c020666b508","value":11034043}},"cf743e501dbc4c9cb182cad6c58b0718":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f28dd2ee39db440fa92842aadc147bdf","placeholder":"​","style":"IPY_MODEL_1dd652c82a6946948098f0b69e51a2b5","value":" 10.5M/10.5M [00:00&lt;00:00, 68.4MB/s]"}},"77144cf2dcb746eab5aeee29ae7a61bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3067e8b271b245e19fd66830a4357187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447138e23d5448d5a437a59006bf712c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cc135fac91e45debca0cb9f323296a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b747ffafa25a4636b3c00c020666b508":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f28dd2ee39db440fa92842aadc147bdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dd652c82a6946948098f0b69e51a2b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0d1883ae923430e8d09eb209f86c8f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55a9a2ab9f474e9eb45fc3dbe58d9f4e","IPY_MODEL_80a322b0dcdd448b864820ce62f063c2","IPY_MODEL_fa8a49970e6740b39fdd5438d2396bea"],"layout":"IPY_MODEL_4f427aaca3834293abde5b7952160f47"}},"55a9a2ab9f474e9eb45fc3dbe58d9f4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4b4e64358ee42de89e8157999407c23","placeholder":"​","style":"IPY_MODEL_a5fb664e72bb4a3ba278003b4abbe3a2","value":"100%"}},"80a322b0dcdd448b864820ce62f063c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c537af76460f45f3b4db31827125de38","max":26096019,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ab99761e74a4febbc3f507bb89f8b51","value":26096019}},"fa8a49970e6740b39fdd5438d2396bea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_455f88205fd241fba8b95851371eacc4","placeholder":"​","style":"IPY_MODEL_ac88ca2995944970a45bdebf024a35de","value":" 24.9M/24.9M [00:00&lt;00:00, 33.8MB/s]"}},"4f427aaca3834293abde5b7952160f47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4b4e64358ee42de89e8157999407c23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5fb664e72bb4a3ba278003b4abbe3a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c537af76460f45f3b4db31827125de38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ab99761e74a4febbc3f507bb89f8b51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"455f88205fd241fba8b95851371eacc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac88ca2995944970a45bdebf024a35de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa6298191ff14708ba8a30c207524815":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_516009b7254e4dfba23ad790723dec38","IPY_MODEL_818e52aeb4ef4f9b8c74030d9bfe2ed9","IPY_MODEL_292369377e744cf5b5cc6f6d2100b2a3"],"layout":"IPY_MODEL_2bc33165de9140bd97076301c3d602d3"}},"516009b7254e4dfba23ad790723dec38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f5c55d3414c4e47b36ebcf4732ad54f","placeholder":"​","style":"IPY_MODEL_5512adbf15b44550a775d4eb91db031a","value":"100%"}},"818e52aeb4ef4f9b8c74030d9bfe2ed9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aff7b14313f241b58e217ed5e3afa621","max":53348075,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9cba3758f0845a49432f8e185911de2","value":53348075}},"292369377e744cf5b5cc6f6d2100b2a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1836b202d424ba48fc4c3bf08436c00","placeholder":"​","style":"IPY_MODEL_3a0168758f2c4ce28afc938560431061","value":" 50.9M/50.9M [00:01&lt;00:00, 30.1MB/s]"}},"2bc33165de9140bd97076301c3d602d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f5c55d3414c4e47b36ebcf4732ad54f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5512adbf15b44550a775d4eb91db031a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aff7b14313f241b58e217ed5e3afa621":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9cba3758f0845a49432f8e185911de2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1836b202d424ba48fc4c3bf08436c00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a0168758f2c4ce28afc938560431061":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ed86e24f8974d8eb89ca62a6e03db85":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8bca282acdf4214a4ff7d852594ba9b","IPY_MODEL_ece10ecd82294bb6b914fe9087decb51","IPY_MODEL_65df467e49244659a49d667cda86c072"],"layout":"IPY_MODEL_a09b55744fa04bd7b7382a39a3068d77"}},"d8bca282acdf4214a4ff7d852594ba9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_226f9e4ae3484208b3d61b6bc4b3c336","placeholder":"​","style":"IPY_MODEL_448bd36cfb1e4240851f8e88aeaa4778","value":"100%"}},"ece10ecd82294bb6b914fe9087decb51":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8afec5767ee5406993a28f9d678f2966","max":96503107,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ef97d4cb3e64f9f871c969b4d058c13","value":96503107}},"65df467e49244659a49d667cda86c072":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ed1a99a1e8c4c09a2658c564a5d14c2","placeholder":"​","style":"IPY_MODEL_d7367c10dc87405190267c22ca971923","value":" 92.0M/92.0M [00:07&lt;00:00, 16.3MB/s]"}},"a09b55744fa04bd7b7382a39a3068d77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"226f9e4ae3484208b3d61b6bc4b3c336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"448bd36cfb1e4240851f8e88aeaa4778":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8afec5767ee5406993a28f9d678f2966":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ef97d4cb3e64f9f871c969b4d058c13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ed1a99a1e8c4c09a2658c564a5d14c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7367c10dc87405190267c22ca971923":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Material Recognition using Transfer Learning on Yolov5\n\nThe goal of this project is to train a convolutional neural network to classify color photographs of surfaces into one of ten common material categories: fabric, foliage, glass, leather, metal, paper, plastic, stone, water, and wood. Some tasks to consider:\n\n1. Modify some previously published architectures e.g., increase the network depth, reducing their parameters, etc.\n2. Try data augmentation to increase the number of training images\n3. Try a larger dataset, Materials in Context Database (MINC)\n\nDataset: Flickr Material Database (FMD)","metadata":{}},{"cell_type":"markdown","source":"The notebook here will focus on transfer learning using yolov5. The overview of this notebook is as follows:\n\n1. Setting up and preparing the notebook environment\n2. Download the ImageNet trained models pretrained on ImageNet using YOLOv5 Utils\n3. Train-val-test split\n4. Train on train dataset for all 5 models (n, s, m, l, x)\n5. Validate on validation dataset for all 5 models (n, s, m, l, x)\n6. Predict on test dataset for all 5 models (n, s, m, l, x)\n7. Calculate test accuracy score for all 5 models (n, s, m, l, x)","metadata":{"id":"5GYQX3of4QiW"}},{"cell_type":"markdown","source":"# Setup","metadata":{"id":"-PJ8vlYXbWtN"}},{"cell_type":"code","source":"import os\n!git clone https://github.com/ultralytics/yolov5  # clone\nos.chdir(\"/kaggle/working\")\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nimport torch\nimport utils\ndisplay = utils.notebook_init()  # checks","metadata":{"id":"pIM7fOwm8A7l","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5231e7f7-21bf-4a30-a894-941fd525c627","execution":{"iopub.status.busy":"2022-11-11T05:46:20.131618Z","iopub.execute_input":"2022-11-11T05:46:20.132028Z","iopub.status.idle":"2022-11-11T05:46:37.321770Z","shell.execute_reply.started":"2022-11-11T05:46:20.131979Z","shell.execute_reply":"2022-11-11T05:46:37.320217Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"YOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","output_type":"stream"},{"name":"stdout","text":"Setup complete ✅ (2 CPUs, 15.6 GB RAM, 3964.5/4030.6 GB disk)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Download the ImageNet trained models pretrained on ImageNet using YOLOv5 Utils","metadata":{"id":"i_DrUi2nmF40"}},{"cell_type":"code","source":"from utils.downloads import attempt_download\n\np5 = ['n', 's', 'm', 'l', 'x']  # P5 models\ncls = [f'{x}-cls' for x in p5]  # classification models\n\nfor x in cls:\n    attempt_download(f'weights/yolov5{x}.pt')","metadata":{"id":"o2scLEh6EYnL","colab":{"base_uri":"https://localhost:8080/","height":351,"referenced_widgets":["57eb0eaa318d4b40a07c22d70472c103","07b3a9ca01b74360b2c7a3d5b47f1fe9","af034548f4664e6a8c4b7530a0f859ea","ed9bb988ff714b308300033581caf93b","de4f8cfa2f1e4f51af2935bd14105322","6eeee67e46644c6c9518043ea5357a28","dbd4418aae6641b0bf4c81b225ff9822","18ebec48a2c34535b86182e837289495","0bbba5c4d4044aa0b9044d7fa30245ed","87ea85d6a1624e04a454d719ccdfcf78","30b7b05f25ac45309555054929ba7333","5f29acf7fe594a81a392c3a43e835c63","87f0057ff99248b5a571f982cf473976","d4d0ef9764854afda3af4403b073d30a","cf743e501dbc4c9cb182cad6c58b0718","77144cf2dcb746eab5aeee29ae7a61bb","3067e8b271b245e19fd66830a4357187","447138e23d5448d5a437a59006bf712c","1cc135fac91e45debca0cb9f323296a2","b747ffafa25a4636b3c00c020666b508","f28dd2ee39db440fa92842aadc147bdf","1dd652c82a6946948098f0b69e51a2b5","b0d1883ae923430e8d09eb209f86c8f5","55a9a2ab9f474e9eb45fc3dbe58d9f4e","80a322b0dcdd448b864820ce62f063c2","fa8a49970e6740b39fdd5438d2396bea","4f427aaca3834293abde5b7952160f47","f4b4e64358ee42de89e8157999407c23","a5fb664e72bb4a3ba278003b4abbe3a2","c537af76460f45f3b4db31827125de38","4ab99761e74a4febbc3f507bb89f8b51","455f88205fd241fba8b95851371eacc4","ac88ca2995944970a45bdebf024a35de","fa6298191ff14708ba8a30c207524815","516009b7254e4dfba23ad790723dec38","818e52aeb4ef4f9b8c74030d9bfe2ed9","292369377e744cf5b5cc6f6d2100b2a3","2bc33165de9140bd97076301c3d602d3","8f5c55d3414c4e47b36ebcf4732ad54f","5512adbf15b44550a775d4eb91db031a","aff7b14313f241b58e217ed5e3afa621","f9cba3758f0845a49432f8e185911de2","c1836b202d424ba48fc4c3bf08436c00","3a0168758f2c4ce28afc938560431061","5ed86e24f8974d8eb89ca62a6e03db85","d8bca282acdf4214a4ff7d852594ba9b","ece10ecd82294bb6b914fe9087decb51","65df467e49244659a49d667cda86c072","a09b55744fa04bd7b7382a39a3068d77","226f9e4ae3484208b3d61b6bc4b3c336","448bd36cfb1e4240851f8e88aeaa4778","8afec5767ee5406993a28f9d678f2966","7ef97d4cb3e64f9f871c969b4d058c13","1ed1a99a1e8c4c09a2658c564a5d14c2","d7367c10dc87405190267c22ca971923"]},"outputId":"29acf707-b2e8-4298-d498-3ae80537a811","execution":{"iopub.status.busy":"2022-11-11T05:46:37.324676Z","iopub.execute_input":"2022-11-11T05:46:37.325584Z","iopub.status.idle":"2022-11-11T05:46:57.727540Z","shell.execute_reply.started":"2022-11-11T05:46:37.325541Z","shell.execute_reply":"2022-11-11T05:46:57.726455Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5n-cls.pt to weights/yolov5n-cls.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/4.87M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"449991d5eea74d399c70ed8086f01adc"}},"metadata":{}},{"name":"stderr","text":"\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s-cls.pt to weights/yolov5s-cls.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/10.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b7b2d5476a45858649ca3b8f55fb07"}},"metadata":{}},{"name":"stderr","text":"\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m-cls.pt to weights/yolov5m-cls.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/24.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9885b88d48f6470d80e82140fccd2206"}},"metadata":{}},{"name":"stderr","text":"\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5l-cls.pt to weights/yolov5l-cls.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/50.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4415890ff8946fdb2b7e00c292841ca"}},"metadata":{}},{"name":"stderr","text":"\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5x-cls.pt to weights/yolov5x-cls.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/92.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b739f66ff22747eaa6971486995db3cb"}},"metadata":{}},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train-val-test split","metadata":{"id":"-5z7Yv42FGrK"}},{"cell_type":"code","source":"!pip install split-folders","metadata":{"id":"z3WIffqYd6T9","outputId":"abbdf810-dc89-4400-c7d2-d58c207e2c27","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2022-11-11T05:46:57.737710Z","iopub.execute_input":"2022-11-11T05:46:57.738494Z","iopub.status.idle":"2022-11-11T05:47:07.452938Z","shell.execute_reply.started":"2022-11-11T05:46:57.738457Z","shell.execute_reply":"2022-11-11T05:47:07.451687Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting split-folders\n  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/input/\")","metadata":{"execution":{"iopub.status.busy":"2022-11-11T05:46:57.729611Z","iopub.execute_input":"2022-11-11T05:46:57.730639Z","iopub.status.idle":"2022-11-11T05:46:57.735931Z","shell.execute_reply.started":"2022-11-11T05:46:57.730600Z","shell.execute_reply":"2022-11-11T05:46:57.734819Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import splitfolders\nsplitfolders.ratio('fmddataset/image', output=\"/kaggle/working/yolov5/output\", seed=1337, ratio=(.7, 0.3)) ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kcNqDLsXhH2C","outputId":"8d22f16d-bca6-4f13-9f55-0a2f931b7f6a","execution":{"iopub.status.busy":"2022-11-11T05:47:07.456394Z","iopub.execute_input":"2022-11-11T05:47:07.457542Z","iopub.status.idle":"2022-11-11T05:47:12.351251Z","shell.execute_reply.started":"2022-11-11T05:47:07.457505Z","shell.execute_reply":"2022-11-11T05:47:12.350242Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Copying files: 1003 files [00:04, 205.97 files/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"splitfolders.ratio('../input/testdata/test/data/test', output=\"/kaggle/working/yolov5/output\", seed=1337, ratio=(0, 0, 1)) ","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:42:25.130034Z","iopub.execute_input":"2022-11-11T06:42:25.130912Z","iopub.status.idle":"2022-11-11T06:42:27.675915Z","shell.execute_reply.started":"2022-11-11T06:42:25.130867Z","shell.execute_reply":"2022-11-11T06:42:27.674908Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Copying files: 30 files [00:02, 11.98 files/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train on train dataset for all 5 models (n, s, m, l, x)","metadata":{}},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")\nfrom utils.downloads import attempt_download\n\n%cd yolov5\n\np5 = ['n', 's', 'm', 'l', 'x']  # P5 models\ncls = [f'{x}-cls' for x in p5]  # classification models\n\nfor x in cls:\n    attempt_download(f'weights/yolov5{x}.pt')\n    \nfor x in p5:\n    !python classify/train.py --model yolov5{x}-cls.pt --data output --epochs 100 --img 224 --pretrained weights/yolov5{x}-cls.pt","metadata":{"id":"MXWTTN2BEaqe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3ca41164-eab5-4139-ea38-76b768c446c4","execution":{"iopub.status.busy":"2022-11-11T05:47:12.352883Z","iopub.execute_input":"2022-11-11T05:47:12.353567Z","iopub.status.idle":"2022-11-11T06:36:43.122222Z","shell.execute_reply.started":"2022-11-11T05:47:12.353527Z","shell.execute_reply":"2022-11-11T06:36:43.121050Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5n-cls.pt, data=output, epochs=100, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5n-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5n-cls.pt to yolov5n-cls.pt...\n100%|██████████████████████████████████████| 4.87M/4.87M [00:01<00:00, 4.47MB/s]\n\nModel summary: 149 layers, 1224810 parameters, 1224810 gradients, 3.0 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp\u001b[0m\nStarting yolov5n-cls.pt training on output dataset with 10 classes for 100 epochs...\n\n     Epoch   GPU_mem  train_loss    val_loss    top1_acc    top5_acc\n     1/100    0.627G        2.35         2.3      0.0993       0.497: 100%|█████\n     2/100    0.975G        2.27        2.31      0.0993       0.497: 100%|█████\n     3/100    0.975G        2.22        2.32      0.0993       0.497: 100%|█████\n     4/100    0.975G        2.15        2.37      0.0993       0.497: 100%|█████\n     5/100    0.975G        2.14        2.36      0.0993       0.497: 100%|█████\n     6/100    0.975G        2.13        2.32       0.109       0.573: 100%|█████\n     7/100    0.975G        2.11        2.29       0.132        0.57: 100%|█████\n     8/100    0.975G        2.08        2.21       0.205       0.666: 100%|█████\n     9/100    0.975G        2.11        2.28       0.192       0.672: 100%|█████\n    10/100    0.975G        2.07        2.18       0.225       0.689: 100%|█████\n    11/100    0.975G        2.04        2.11       0.228       0.735: 100%|█████\n    12/100    0.975G        2.04        2.06       0.265       0.752: 100%|█████\n    13/100    0.975G        2.02        2.11       0.275       0.738: 100%|█████\n    14/100    0.975G        2.02        2.11       0.262       0.762: 100%|█████\n    15/100    0.975G        1.99        2.07       0.258       0.738: 100%|█████\n    16/100    0.975G           2        2.27       0.235       0.675: 100%|█████\n    17/100    0.975G        2.01        2.32       0.242       0.675: 100%|█████\n    18/100    0.975G           2        2.11       0.275       0.765: 100%|█████\n    19/100    0.975G        1.99        2.14       0.265       0.715: 100%|█████\n    20/100    0.975G           2        2.14       0.255       0.725: 100%|█████\n    21/100    0.975G        1.96        2.03       0.298       0.755: 100%|█████\n    22/100    0.975G        1.93        2.06       0.291       0.762: 100%|█████\n    23/100    0.975G        1.93        2.08       0.278       0.768: 100%|█████\n    24/100    0.975G        1.95        2.04       0.275       0.772: 100%|█████\n    25/100    0.975G        1.94        2.02       0.334       0.748: 100%|█████\n    26/100    0.975G        1.94        1.96       0.331       0.798: 100%|█████\n    27/100    0.975G        1.93        2.01       0.325       0.781: 100%|█████\n    28/100    0.975G        1.88        1.97       0.328       0.798: 100%|█████\n    29/100    0.975G        1.88           2       0.311       0.788: 100%|█████\n    30/100    0.975G        1.91        1.98       0.318       0.801: 100%|█████\n    31/100    0.975G        1.89        1.94       0.348       0.818: 100%|█████\n    32/100    0.975G        1.84        1.96       0.288       0.805: 100%|█████\n    33/100    0.975G        1.83        2.03       0.315       0.765: 100%|█████\n    34/100    0.975G        1.84        2.07       0.258       0.762: 100%|█████\n    35/100    0.975G        1.83        1.99       0.315       0.825: 100%|█████\n    36/100    0.975G        1.83        1.98       0.351       0.801: 100%|█████\n    37/100    0.975G        1.84        1.97       0.341       0.765: 100%|█████\n    38/100    0.975G        1.81        1.97       0.334       0.805: 100%|█████\n    39/100    0.975G         1.8        1.99       0.311       0.805: 100%|█████\n    40/100    0.975G        1.84           2       0.318       0.801: 100%|█████\n    41/100    0.975G        1.75        2.01       0.351       0.811: 100%|█████\n    42/100    0.975G         1.8        2.01       0.331       0.801: 100%|█████\n    43/100    0.975G        1.82        2.03       0.348       0.808: 100%|█████\n    44/100    0.975G        1.78        2.05       0.321       0.801: 100%|█████\n    45/100    0.975G        1.73        1.93       0.374       0.828: 100%|█████\n    46/100    0.975G        1.74        2.01       0.328       0.805: 100%|█████\n    47/100    0.975G        1.77         1.9       0.394       0.825: 100%|█████\n    48/100    0.975G        1.71         1.9       0.387       0.808: 100%|█████\n    49/100    0.975G        1.75        2.07       0.341       0.791: 100%|█████\n    50/100    0.975G        1.76        2.07       0.344       0.781: 100%|█████\n    51/100    0.975G         1.7        1.98       0.358       0.818: 100%|█████\n    52/100    0.975G        1.71           2       0.364       0.825: 100%|█████\n    53/100    0.975G        1.67        1.95       0.364       0.818: 100%|█████\n    54/100    0.975G        1.71        1.99       0.351       0.821: 100%|█████\n    55/100    0.975G        1.66        1.97       0.331       0.818: 100%|█████\n    56/100    0.975G        1.64           2       0.414       0.831: 100%|█████\n    57/100    0.975G        1.61        2.01       0.341       0.811: 100%|█████\n    58/100    0.975G        1.64        1.97       0.384       0.805: 100%|█████\n    59/100    0.975G        1.62        1.98       0.368       0.808: 100%|█████\n    60/100    0.975G        1.61        1.98       0.397       0.825: 100%|█████\n    61/100    0.975G        1.59        1.87       0.401       0.844: 100%|█████\n    62/100    0.975G        1.59        1.96       0.377       0.848: 100%|█████\n    63/100    0.975G        1.56        2.02       0.364       0.821: 100%|█████\n    64/100    0.975G        1.57        2.07       0.368       0.785: 100%|█████\n    65/100    0.975G        1.57        2.01       0.421       0.791: 100%|█████\n    66/100    0.975G        1.55        2.16       0.364       0.828: 100%|█████\n    67/100    0.975G        1.58        1.93       0.411       0.825: 100%|█████\n    68/100    0.975G        1.49        1.94       0.407       0.848: 100%|█████\n    69/100    0.975G        1.52        2.05       0.364       0.815: 100%|█████\n    70/100    0.975G        1.52        1.95       0.381       0.848: 100%|█████\n    71/100    0.975G        1.52        1.98       0.387       0.831: 100%|█████\n    72/100    0.975G        1.51           2       0.404       0.858: 100%|█████\n    73/100    0.975G        1.48        2.01       0.384       0.834: 100%|█████\n    74/100    0.975G        1.53        1.97       0.377       0.841: 100%|█████\n    75/100    0.975G        1.48        2.04       0.387       0.815: 100%|█████\n    76/100    0.975G        1.47         1.9       0.417       0.838: 100%|█████\n    77/100    0.975G        1.44        1.89       0.407       0.858: 100%|█████\n    78/100    0.975G        1.46        1.96       0.421       0.861: 100%|█████\n    79/100    0.975G        1.44        2.01       0.387       0.838: 100%|█████\n    80/100    0.975G        1.48        1.99       0.368       0.844: 100%|█████\n    81/100    0.975G        1.47           2       0.381       0.861: 100%|█████\n    82/100    0.975G         1.4        1.94       0.404       0.864: 100%|█████\n    83/100    0.975G         1.4        1.95       0.401       0.851: 100%|█████\n    84/100    0.975G        1.42        1.91       0.381       0.851: 100%|█████\n    85/100    0.975G        1.36        1.95       0.407       0.877: 100%|█████\n    86/100    0.975G        1.39        1.93       0.411       0.877: 100%|█████\n    87/100    0.975G        1.37        1.95       0.391       0.831: 100%|█████\n    88/100    0.975G        1.34        1.95       0.417       0.861: 100%|█████\n    89/100    0.975G        1.33        2.01       0.394       0.851: 100%|█████\n    90/100    0.975G        1.33        1.97       0.417       0.851: 100%|█████\n    91/100    0.975G         1.3        1.93       0.424       0.851: 100%|█████\n    92/100    0.975G        1.31        2.02       0.404       0.871: 100%|█████\n    93/100    0.975G        1.33           2       0.411       0.858: 100%|█████\n    94/100    0.975G        1.32         1.9       0.434       0.858: 100%|█████\n    95/100    0.975G        1.31        1.96       0.444       0.868: 100%|█████\n    96/100    0.975G        1.32        1.91       0.421       0.871: 100%|█████\n    97/100    0.975G        1.29        1.96       0.424       0.871: 100%|█████\n    98/100    0.975G        1.28        1.99       0.414       0.874: 100%|█████\n    99/100    0.975G        1.26        1.93       0.411       0.858: 100%|█████\n   100/100    0.975G        1.27        1.99       0.417       0.851: 100%|█████\n\nTraining complete (0.125 hours)\nResults saved to \u001b[1mruns/train-cls/exp\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data output\nExport:          python export.py --weights runs/train-cls/exp/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp/weights/best.pt')\nVisualize:       https://netron.app\n\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f960678bb00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f960678bb00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5s-cls.pt, data=output, epochs=100, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s-cls.pt to yolov5s-cls.pt...\n100%|██████████████████████████████████████| 10.5M/10.5M [00:00<00:00, 38.4MB/s]\n\nModel summary: 149 layers, 4185290 parameters, 4185290 gradients, 10.5 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp2\u001b[0m\nStarting yolov5s-cls.pt training on output dataset with 10 classes for 100 epochs...\n\n     Epoch   GPU_mem  train_loss    val_loss    top1_acc    top5_acc\n     1/100     1.54G         2.4        2.31      0.0993       0.497: 100%|█████\n     2/100     1.54G        2.25        2.31      0.0993       0.497: 100%|█████\n     3/100     1.54G        2.22        2.36      0.0993       0.497: 100%|█████\n     4/100     1.54G        2.17         2.5       0.103       0.497: 100%|█████\n     5/100     1.54G        2.18         2.4       0.129        0.53: 100%|█████\n     6/100     1.54G        2.16         2.3       0.175       0.616: 100%|█████\n     7/100     1.54G        2.14        2.22       0.199       0.662: 100%|█████\n     8/100     1.54G        2.15        2.19       0.205       0.712: 100%|█████\n     9/100     1.54G        2.11        2.33       0.209       0.675: 100%|█████\n    10/100     1.54G        2.13         2.4       0.192       0.679: 100%|█████\n    11/100     1.54G        2.09        2.32       0.205       0.682: 100%|█████\n    12/100     1.54G        2.07        2.16       0.248       0.742: 100%|█████\n    13/100     1.54G        2.07        2.13       0.238       0.722: 100%|█████\n    14/100     1.54G        2.08         2.1       0.262       0.705: 100%|█████\n    15/100     1.54G        2.03        2.18       0.219       0.692: 100%|█████\n    16/100     1.54G        2.05        2.16       0.212       0.768: 100%|█████\n    17/100     1.54G        2.07        2.21       0.262       0.689: 100%|█████\n    18/100     1.54G        2.07        2.12       0.255       0.778: 100%|█████\n    19/100     1.54G        2.03        2.05       0.255       0.755: 100%|█████\n    20/100     1.54G        2.06        2.12       0.252       0.745: 100%|█████\n    21/100     1.54G        2.05        2.15       0.245       0.748: 100%|█████\n    22/100     1.54G        2.01        2.27       0.238       0.728: 100%|█████\n    23/100     1.54G           2        2.17       0.275       0.738: 100%|█████\n    24/100     1.54G        2.02        2.08       0.258       0.785: 100%|█████\n    25/100     1.54G        2.01        2.16       0.225       0.768: 100%|█████\n    26/100     1.54G           2        2.07       0.265       0.781: 100%|█████\n    27/100     1.54G           2        2.08       0.278       0.798: 100%|█████\n    28/100     1.54G        1.97        2.14       0.255       0.755: 100%|█████\n    29/100     1.54G        1.97        2.08       0.268       0.781: 100%|█████\n    30/100     1.54G        1.99        2.04       0.298       0.785: 100%|█████\n    31/100     1.54G        1.96        2.04       0.272       0.801: 100%|█████\n    32/100     1.54G        1.97        2.06       0.275       0.821: 100%|█████\n    33/100     1.54G        1.95        2.32       0.242       0.715: 100%|█████\n    34/100     1.54G        1.94        2.09       0.262       0.791: 100%|█████\n    35/100     1.54G        1.98        2.06       0.275       0.755: 100%|█████\n    36/100     1.54G        1.94        2.02       0.311       0.808: 100%|█████\n    37/100     1.54G        1.97        2.12       0.245       0.742: 100%|█████\n    38/100     1.54G        1.96        2.31       0.209       0.699: 100%|█████\n    39/100     1.54G        1.96        2.21       0.219       0.715: 100%|█████\n    40/100     1.54G        1.95        2.28       0.232       0.722: 100%|█████\n    41/100     1.54G         1.9        2.09       0.278       0.765: 100%|█████\n    42/100     1.54G        1.93        2.03       0.305       0.781: 100%|█████\n    43/100     1.54G        1.95        2.03       0.321       0.788: 100%|█████\n    44/100     1.54G        1.93        2.05       0.262       0.785: 100%|█████\n    45/100     1.54G        1.91        2.01       0.298       0.811: 100%|█████\n    46/100     1.54G         1.9        2.01       0.318       0.778: 100%|█████\n    47/100     1.54G        1.94        2.02       0.338       0.772: 100%|█████\n    48/100     1.54G        1.89           2       0.295       0.785: 100%|█████\n    49/100     1.54G        1.88        2.19       0.242       0.738: 100%|█████\n    50/100     1.54G        1.92           2       0.341       0.808: 100%|█████\n    51/100     1.54G         1.9           2       0.295       0.775: 100%|█████\n    52/100     1.54G        1.86         2.1       0.288       0.768: 100%|█████\n    53/100     1.54G        1.89        1.96       0.331       0.821: 100%|█████\n    54/100     1.54G         1.9        2.06       0.281       0.775: 100%|█████\n    55/100     1.54G        1.87        1.98       0.331       0.811: 100%|█████\n    56/100     1.54G        1.86        2.13       0.272       0.778: 100%|█████\n    57/100     1.54G        1.82        1.98       0.361       0.808: 100%|█████\n    58/100     1.54G        1.85        2.03       0.331       0.791: 100%|█████\n    59/100     1.54G        1.84           2       0.311       0.811: 100%|█████\n    60/100     1.54G        1.82        2.11       0.275       0.765: 100%|█████\n    61/100     1.54G         1.8        1.95       0.325       0.811: 100%|█████\n    62/100     1.54G        1.81        2.05       0.331       0.831: 100%|█████\n    63/100     1.54G        1.82        2.05       0.334       0.772: 100%|█████\n    64/100     1.54G         1.8           2       0.334       0.825: 100%|█████\n    65/100     1.54G        1.79        2.03       0.321       0.795: 100%|█████\n    66/100     1.54G        1.82           2       0.328       0.828: 100%|█████\n    67/100     1.54G        1.79        2.18       0.315       0.758: 100%|█████\n    68/100     1.54G        1.78        1.95       0.348       0.838: 100%|█████\n    69/100     1.54G        1.82        2.21       0.262       0.775: 100%|█████\n    70/100     1.54G        1.82        2.03       0.315       0.798: 100%|█████\n    71/100     1.54G        1.77        1.94       0.368       0.838: 100%|█████\n    72/100     1.54G        1.78        1.95       0.334       0.854: 100%|█████\n    73/100     1.54G        1.78        2.19       0.295       0.775: 100%|█████\n    74/100     1.54G        1.77           2       0.338       0.818: 100%|█████\n    75/100     1.54G        1.76        2.12       0.321       0.791: 100%|█████\n    76/100     1.54G        1.73         2.1       0.344       0.785: 100%|█████\n    77/100     1.54G        1.75        1.92       0.328       0.831: 100%|█████\n    78/100     1.54G        1.78        2.01       0.328       0.818: 100%|█████\n    79/100     1.54G        1.71        1.98       0.348       0.818: 100%|█████\n    80/100     1.54G        1.73        1.92       0.341       0.834: 100%|█████\n    81/100     1.54G        1.72        1.96       0.368       0.838: 100%|█████\n    82/100     1.54G        1.73        1.93       0.361       0.834: 100%|█████\n    83/100     1.54G        1.69        1.99       0.368       0.838: 100%|█████\n    84/100     1.54G        1.73        1.89       0.361       0.844: 100%|█████\n    85/100     1.54G        1.67        1.97       0.344       0.828: 100%|█████\n    86/100     1.54G        1.64        2.03       0.351       0.838: 100%|█████\n    87/100     1.54G         1.7        2.01       0.384       0.801: 100%|█████\n    88/100     1.54G        1.65        1.93       0.368       0.858: 100%|█████\n    89/100     1.54G        1.66        2.03       0.358       0.798: 100%|█████\n    90/100     1.54G        1.62        1.96       0.328       0.844: 100%|█████\n    91/100     1.54G        1.61        1.94       0.371       0.834: 100%|█████\n    92/100     1.54G         1.6        2.04       0.361       0.815: 100%|█████\n    93/100     1.54G        1.63        1.98       0.364       0.838: 100%|█████\n    94/100     1.54G        1.64        1.92       0.377       0.838: 100%|█████\n    95/100     1.54G        1.58        2.04       0.384       0.821: 100%|█████\n    96/100     1.54G        1.63        1.94       0.394       0.841: 100%|█████\n    97/100     1.54G        1.62         1.9       0.397       0.834: 100%|█████\n    98/100     1.54G        1.59           2       0.384       0.841: 100%|█████\n    99/100     1.54G        1.57         1.9       0.384       0.828: 100%|█████\n   100/100     1.54G        1.58        1.97       0.391       0.831: 100%|█████\n\nTraining complete (0.128 hours)\nResults saved to \u001b[1mruns/train-cls/exp2\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp2/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp2/weights/best.pt --data output\nExport:          python export.py --weights runs/train-cls/exp2/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp2/weights/best.pt')\nVisualize:       https://netron.app\n\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f37bab80b00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f37bab80b00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5m-cls.pt, data=output, epochs=100, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5m-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5m-cls.pt to yolov5m-cls.pt...\n100%|██████████████████████████████████████| 24.9M/24.9M [00:01<00:00, 13.8MB/s]\n\nModel summary: 212 layers, 11689450 parameters, 11689450 gradients, 30.9 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 46 weight(decay=0.0), 47 weight(decay=5e-05), 47 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp3\u001b[0m\nStarting yolov5m-cls.pt training on output dataset with 10 classes for 100 epochs...\n\n     Epoch   GPU_mem  train_loss    val_loss    top1_acc    top5_acc\n     1/100     1.97G        2.47         2.3      0.0993         0.5: 100%|█████\n     2/100     2.72G        2.29        2.32      0.0993       0.497: 100%|█████\n     3/100     2.72G        2.27        2.45      0.0993       0.497: 100%|█████\n     4/100     2.72G        2.22        2.52       0.103       0.503: 100%|█████\n     5/100     2.72G        2.21        2.41       0.109        0.53: 100%|█████\n     6/100     2.72G         2.2        2.48       0.132        0.57: 100%|█████\n     7/100     2.72G         2.2        2.53       0.142       0.589: 100%|█████\n     8/100     2.72G        2.17        2.89       0.172       0.593: 100%|█████\n     9/100     2.72G         2.2        2.72       0.172       0.669: 100%|█████\n    10/100     2.72G        2.18         3.2       0.149       0.629: 100%|█████\n    11/100     2.72G        2.15        2.65       0.132       0.623: 100%|█████\n    12/100     2.72G        2.13        2.53       0.169       0.675: 100%|█████\n    13/100     2.72G        2.14        2.19       0.185       0.685: 100%|█████\n    14/100     2.72G        2.14        2.22       0.162       0.689: 100%|█████\n    15/100     2.72G         2.1         2.2       0.185       0.656: 100%|█████\n    16/100     2.72G        2.13        2.14       0.212       0.738: 100%|█████\n    17/100     2.72G        2.12        2.27       0.225       0.695: 100%|█████\n    18/100     2.72G        2.13        2.19       0.209       0.728: 100%|█████\n    19/100     2.72G        2.13        2.16       0.215       0.712: 100%|█████\n    20/100     2.72G        2.12        2.09       0.245       0.715: 100%|█████\n    21/100     2.72G        2.13        2.26       0.149       0.705: 100%|█████\n    22/100     2.72G        2.09        2.13       0.232       0.699: 100%|█████\n    23/100     2.72G        2.09        2.13       0.252       0.722: 100%|█████\n    24/100     2.72G        2.12        2.16       0.238       0.705: 100%|█████\n    25/100     2.72G        2.09        2.19       0.222       0.692: 100%|█████\n    26/100     2.72G        2.11         2.1       0.252       0.748: 100%|█████\n    27/100     2.72G         2.1        2.13       0.238       0.758: 100%|█████\n    28/100     2.72G        2.09        2.15       0.225       0.732: 100%|█████\n    29/100     2.72G        2.06        2.08       0.255       0.788: 100%|█████\n    30/100     2.72G        2.05        2.16       0.242       0.715: 100%|█████\n    31/100     2.72G        2.04        2.09       0.308       0.788: 100%|█████\n    32/100     2.72G        2.03        2.22       0.222       0.719: 100%|█████\n    33/100     2.72G        2.06        2.17       0.238       0.775: 100%|█████\n    34/100     2.72G        2.04        2.34       0.228       0.762: 100%|█████\n    35/100     2.72G        2.04        2.35       0.162       0.662: 100%|█████\n    36/100     2.72G        2.05        2.06       0.315       0.811: 100%|█████\n    37/100     2.72G        2.05        2.02       0.268       0.781: 100%|█████\n    38/100     2.72G        2.02         2.2       0.242       0.762: 100%|█████\n    39/100     2.72G        2.03        2.11       0.228       0.768: 100%|█████\n    40/100     2.72G        2.05        2.08       0.285       0.791: 100%|█████\n    41/100     2.72G        2.02        2.17       0.262       0.725: 100%|█████\n    42/100     2.72G        2.02        2.27       0.242       0.752: 100%|█████\n    43/100     2.72G        2.02        2.24       0.222       0.772: 100%|█████\n    44/100     2.72G        2.01        2.38       0.275       0.728: 100%|█████\n    45/100     2.72G        1.96         2.3       0.235       0.735: 100%|█████\n    46/100     2.72G        1.98        2.05       0.275       0.778: 100%|█████\n    47/100     2.72G        2.02        2.05       0.278       0.775: 100%|█████\n    48/100     2.72G        1.98           2       0.308       0.811: 100%|█████\n    49/100     2.72G        1.96        2.14       0.255       0.758: 100%|█████\n    50/100     2.72G        1.99        2.04       0.275       0.772: 100%|█████\n    51/100     2.72G        1.98        2.15       0.242       0.755: 100%|█████\n    52/100     2.72G        1.99        2.29       0.275       0.742: 100%|█████\n    53/100     2.72G           2        2.03       0.281       0.785: 100%|█████\n    54/100     2.72G        1.96           2       0.311       0.768: 100%|█████\n    55/100     2.72G        1.98        2.04       0.258       0.775: 100%|█████\n    56/100     2.72G        1.95        2.12       0.258       0.768: 100%|█████\n    57/100     2.72G        1.94           2       0.305       0.801: 100%|█████\n    58/100     2.72G        1.94        2.04       0.272       0.775: 100%|█████\n    59/100     2.72G        1.96        2.06       0.278       0.775: 100%|█████\n    60/100     2.72G        1.92        2.07       0.242       0.778: 100%|█████\n    61/100     2.72G        1.92        1.97       0.298       0.815: 100%|█████\n    62/100     2.72G        1.95        2.02       0.311       0.791: 100%|█████\n    63/100     2.72G        1.95        2.13       0.295       0.742: 100%|█████\n    64/100     2.72G        1.93        2.03       0.301       0.808: 100%|█████\n    65/100     2.72G        1.92        2.02       0.318       0.795: 100%|█████\n    66/100     2.72G        1.94         2.1       0.258       0.791: 100%|█████\n    67/100     2.72G        1.92        1.99       0.331       0.828: 100%|█████\n    68/100     2.72G        1.91        2.06       0.301       0.765: 100%|█████\n    69/100     2.72G        1.95        2.08       0.278       0.785: 100%|█████\n    70/100     2.72G        1.93        2.03       0.291       0.805: 100%|█████\n    71/100     2.72G        1.94        1.97       0.334       0.781: 100%|█████\n    72/100     2.72G        1.91        1.98       0.315       0.805: 100%|█████\n    73/100     2.72G        1.87        2.13       0.265       0.752: 100%|█████\n    74/100     2.72G        1.91        2.09       0.288       0.778: 100%|█████\n    75/100     2.72G        1.89        1.98       0.318       0.815: 100%|█████\n    76/100     2.72G         1.9        1.95       0.348       0.798: 100%|█████\n    77/100     2.72G        1.86         1.9       0.331       0.828: 100%|█████\n    78/100     2.72G        1.88        2.01       0.288       0.834: 100%|█████\n    79/100     2.72G        1.88        1.95       0.315       0.828: 100%|█████\n    80/100     2.72G        1.87        1.95       0.321       0.785: 100%|█████\n    81/100     2.72G         1.9        2.06       0.291       0.755: 100%|█████\n    82/100     2.72G        1.88           2       0.334       0.765: 100%|█████\n    83/100     2.72G        1.85           2       0.318       0.808: 100%|█████\n    84/100     2.72G        1.88        1.92       0.328       0.821: 100%|█████\n    85/100     2.72G        1.85        1.98       0.325       0.825: 100%|█████\n    86/100     2.72G        1.85        2.04       0.308       0.778: 100%|█████\n    87/100     2.72G        1.86         1.9       0.328       0.828: 100%|█████\n    88/100     2.72G        1.83         1.9       0.344       0.831: 100%|█████\n    89/100     2.72G        1.83           2       0.325       0.818: 100%|█████\n    90/100     2.72G        1.83        1.93       0.344       0.834: 100%|█████\n    91/100     2.72G        1.82        1.92       0.358       0.828: 100%|█████\n    92/100     2.72G        1.83        1.97       0.338       0.808: 100%|█████\n    93/100     2.72G        1.83        1.92       0.351       0.805: 100%|█████\n    94/100     2.72G        1.79        1.96       0.325       0.805: 100%|█████\n    95/100     2.72G        1.81        1.95       0.315       0.821: 100%|█████\n    96/100     2.72G        1.85        1.96       0.331       0.848: 100%|█████\n    97/100     2.72G        1.82        1.95       0.338       0.821: 100%|█████\n    98/100     2.72G        1.81        1.97       0.358       0.821: 100%|█████\n    99/100     2.72G        1.79        1.91       0.351       0.828: 100%|█████\n   100/100     2.72G         1.8        1.93       0.364       0.831: 100%|█████\n\nTraining complete (0.139 hours)\nResults saved to \u001b[1mruns/train-cls/exp3\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp3/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp3/weights/best.pt --data output\nExport:          python export.py --weights runs/train-cls/exp3/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp3/weights/best.pt')\nVisualize:       https://netron.app\n\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f931eeedb00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f931eeedb00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5l-cls.pt, data=output, epochs=100, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5l-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5l-cls.pt to yolov5l-cls.pt...\n100%|██████████████████████████████████████| 50.9M/50.9M [00:02<00:00, 25.4MB/s]\n\nModel summary: 275 layers, 25284490 parameters, 25284490 gradients, 68.8 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 60 weight(decay=0.0), 61 weight(decay=5e-05), 61 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp4\u001b[0m\nStarting yolov5l-cls.pt training on output dataset with 10 classes for 100 epochs...\n\n     Epoch   GPU_mem  train_loss    val_loss    top1_acc    top5_acc\n     1/100     3.59G        2.48         2.3      0.0993       0.503: 100%|█████\n     2/100     4.16G        2.36         2.3       0.103         0.5: 100%|█████\n     3/100     4.16G        2.34        2.32       0.103       0.503: 100%|█████\n     4/100     4.16G        2.32        2.95      0.0927       0.503: 100%|█████\n     5/100     4.16G        2.33         2.3      0.0828       0.507: 100%|█████\n     6/100     4.16G         2.3         2.3       0.129       0.533: 100%|█████\n     7/100     4.16G        2.28        2.29      0.0993       0.526: 100%|█████\n     8/100     4.16G        2.27        2.27       0.152       0.619: 100%|█████\n     9/100     4.16G        2.25        2.28       0.149       0.579: 100%|█████\n    10/100     4.16G        2.25         2.3       0.149       0.639: 100%|█████\n    11/100     4.16G        2.22        2.43       0.175       0.692: 100%|█████\n    12/100     4.16G        2.18        2.35       0.166       0.632: 100%|█████\n    13/100     4.16G        2.19        2.28       0.169       0.649: 100%|█████\n    14/100     4.16G        2.16         2.3       0.175       0.656: 100%|█████\n    15/100     4.16G        2.13        3.21       0.132       0.609: 100%|█████\n    16/100     4.16G        2.17        3.67       0.142       0.669: 100%|█████\n    17/100     4.16G        2.16        2.73       0.166       0.603: 100%|█████\n    18/100     4.16G        2.14        2.64       0.142       0.636: 100%|█████\n    19/100     4.16G        2.15        2.51       0.162       0.543: 100%|█████\n    20/100     4.16G        2.14        2.49       0.182       0.606: 100%|█████\n    21/100     4.16G        2.14        2.35       0.162       0.679: 100%|█████\n    22/100     4.16G        2.12        2.24       0.182       0.662: 100%|█████\n    23/100     4.16G        2.12        2.71       0.195       0.692: 100%|█████\n    24/100     4.16G        2.18        2.26       0.166       0.682: 100%|█████\n    25/100     4.16G        2.12        2.19       0.195       0.715: 100%|█████\n    26/100     4.16G        2.13        2.19       0.199       0.715: 100%|█████\n    27/100     4.16G        2.11        2.11       0.238       0.728: 100%|█████\n    28/100     4.16G        2.12        2.11       0.232       0.702: 100%|█████\n    29/100     4.16G         2.1         2.1       0.255       0.732: 100%|█████\n    30/100     4.16G        2.11        2.13       0.225       0.752: 100%|█████\n    31/100     4.16G        2.09         2.1       0.232       0.728: 100%|█████\n    32/100     4.16G        2.08        2.15       0.235       0.735: 100%|█████\n    33/100     4.16G        2.09        2.11       0.252       0.738: 100%|█████\n    34/100     4.16G        2.06        2.16       0.205       0.725: 100%|█████\n    35/100     4.16G        2.09        2.14       0.215       0.742: 100%|█████\n    36/100     4.16G        2.11        2.15       0.248       0.735: 100%|█████\n    37/100     4.16G         2.1        2.06       0.262       0.778: 100%|█████\n    38/100     4.16G        2.09         2.1       0.245       0.745: 100%|█████\n    39/100     4.16G        2.08        2.04       0.268       0.798: 100%|█████\n    40/100     4.16G         2.1        2.06       0.258       0.785: 100%|█████\n    41/100     4.16G        2.05        2.11       0.262       0.781: 100%|█████\n    42/100     4.16G        2.05        2.15       0.195       0.732: 100%|█████\n    43/100     4.16G        2.08         2.1       0.202       0.745: 100%|█████\n    44/100     4.16G        2.07        2.12       0.222       0.775: 100%|█████\n    45/100     4.16G        2.05        2.12       0.238       0.775: 100%|█████\n    46/100     4.16G        2.05        2.08       0.252       0.781: 100%|█████\n    47/100     4.16G        2.06        2.12       0.238       0.738: 100%|█████\n    48/100     4.16G        2.04        2.18       0.262       0.712: 100%|█████\n    49/100     4.16G        2.04        2.07       0.245       0.752: 100%|█████\n    50/100     4.16G        2.06        2.08       0.242       0.765: 100%|█████\n    51/100     4.16G        2.07        2.01       0.301       0.755: 100%|█████\n    52/100     4.16G        2.05         2.1       0.275       0.748: 100%|█████\n    53/100     4.16G        2.04        2.08       0.248       0.758: 100%|█████\n    54/100     4.16G        2.03        2.03       0.305       0.765: 100%|█████\n    55/100     4.16G        2.04        2.09       0.268       0.755: 100%|█████\n    56/100     4.16G        2.04        2.09       0.258       0.765: 100%|█████\n    57/100     4.16G           2        2.06       0.268       0.768: 100%|█████\n    58/100     4.16G        2.03        2.05       0.291       0.768: 100%|█████\n    59/100     4.16G        2.03        2.13       0.298       0.728: 100%|█████\n    60/100     4.16G        2.03        2.07       0.272       0.781: 100%|█████\n    61/100     4.16G        2.03        2.05       0.245       0.765: 100%|█████\n    62/100     4.16G        2.01        2.06       0.268       0.791: 100%|█████\n    63/100     4.16G        2.03        2.09       0.262       0.775: 100%|█████\n    64/100     4.16G        2.03        2.08       0.281       0.781: 100%|█████\n    65/100     4.16G        2.02        2.04       0.262       0.788: 100%|█████\n    66/100     4.16G        2.01         2.1       0.242       0.781: 100%|█████\n    67/100     4.16G        2.02        2.11       0.248       0.772: 100%|█████\n    68/100     4.16G        2.01        2.11       0.238       0.775: 100%|█████\n    69/100     4.16G        2.03        2.07       0.252       0.795: 100%|█████\n    70/100     4.16G           2        2.06       0.321       0.768: 100%|█████\n    71/100     4.16G        2.02           2       0.331       0.785: 100%|█████\n    72/100     4.16G        1.98           2       0.325       0.772: 100%|█████\n    73/100     4.16G        1.96        2.07       0.268       0.772: 100%|█████\n    74/100     4.16G        2.01        2.01       0.298       0.811: 100%|█████\n    75/100     4.16G        1.97        1.99       0.318       0.785: 100%|█████\n    76/100     4.16G        1.99        2.01       0.315       0.762: 100%|█████\n    77/100     4.16G        1.97        2.04       0.318       0.748: 100%|█████\n    78/100     4.16G        1.97        1.99       0.334       0.785: 100%|█████\n    79/100     4.16G        1.95        2.09       0.281       0.772: 100%|█████\n    80/100     4.16G           2        2.04       0.308       0.755: 100%|█████\n    81/100     4.16G        1.98        2.03       0.305       0.742: 100%|█████\n    82/100     4.16G        1.96        2.01       0.275       0.795: 100%|█████\n    83/100     4.16G        1.93        2.03       0.278       0.801: 100%|█████\n    84/100     4.16G        1.97        2.04       0.278       0.728: 100%|█████\n    85/100     4.16G        1.94        2.03       0.338       0.795: 100%|█████\n    86/100     4.16G        1.94        2.07       0.301       0.781: 100%|█████\n    87/100     4.16G        1.97        2.01       0.331       0.795: 100%|█████\n    88/100     4.16G        1.94           2       0.325       0.795: 100%|█████\n    89/100     4.16G        1.92        1.96       0.364       0.811: 100%|█████\n    90/100     4.16G        1.93        1.95       0.321       0.805: 100%|█████\n    91/100     4.16G        1.93        2.03       0.298       0.811: 100%|█████\n    92/100     4.16G         1.9        2.01       0.334       0.828: 100%|█████\n    93/100     4.16G        1.92        1.97       0.334       0.805: 100%|█████\n    94/100     4.16G        1.91        1.99       0.334       0.801: 100%|█████\n    95/100     4.16G        1.88        1.96       0.341       0.825: 100%|█████\n    96/100     4.16G        1.93        1.94       0.331       0.831: 100%|█████\n    97/100     4.16G        1.91        1.96       0.328       0.821: 100%|█████\n    98/100     4.16G         1.9        2.01       0.311       0.808: 100%|█████\n    99/100     4.16G        1.91        1.95       0.311       0.818: 100%|█████\n   100/100     4.16G        1.91        1.96       0.348       0.811: 100%|█████\n\nTraining complete (0.153 hours)\nResults saved to \u001b[1mruns/train-cls/exp4\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp4/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp4/weights/best.pt --data output\nExport:          python export.py --weights runs/train-cls/exp4/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp4/weights/best.pt')\nVisualize:       https://netron.app\n\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb2239c5b00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fb2239c5b00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5x-cls.pt, data=output, epochs=100, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5x-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5x-cls.pt to yolov5x-cls.pt...\n100%|██████████████████████████████████████| 92.0M/92.0M [00:03<00:00, 31.3MB/s]\n\nModel summary: 338 layers, 46828970 parameters, 46828970 gradients, 129.6 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 74 weight(decay=0.0), 75 weight(decay=5e-05), 75 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp5\u001b[0m\nStarting yolov5x-cls.pt training on output dataset with 10 classes for 100 epochs...\n\n     Epoch   GPU_mem  train_loss    val_loss    top1_acc    top5_acc\n     1/100     4.96G        2.44         2.3       0.103         0.5: 100%|█████\n     2/100     5.72G        2.34         2.3       0.103         0.5: 100%|█████\n     3/100     5.72G        2.35         2.3       0.103         0.5: 100%|█████\n     4/100     5.72G        2.31         2.3      0.0993         0.5: 100%|█████\n     5/100     5.72G        2.32        3.69       0.109        0.53: 100%|█████\n     6/100     5.72G        2.32        2.32       0.106        0.51: 100%|█████\n     7/100     5.72G        2.31        2.32       0.096       0.533: 100%|█████\n     8/100     5.72G         2.3        2.31       0.096         0.5: 100%|█████\n     9/100     5.72G         2.3        2.45       0.103         0.5: 100%|█████\n    10/100     5.72G         2.3         2.6       0.109       0.536: 100%|█████\n    11/100     5.72G        2.29        2.31       0.116       0.493: 100%|█████\n    12/100     5.72G        2.24        2.27       0.132       0.609: 100%|█████\n    13/100     5.72G        2.22        2.24       0.195       0.649: 100%|█████\n    14/100     5.72G        2.27        2.28       0.156       0.619: 100%|█████\n    15/100     5.72G        2.21        2.26       0.172       0.623: 100%|█████\n    16/100     5.72G        2.19        2.23       0.185       0.695: 100%|█████\n    17/100     5.72G        2.17        2.27       0.156       0.596: 100%|█████\n    18/100     5.72G        2.17        2.18       0.189       0.682: 100%|█████\n    19/100     5.72G        2.16        2.16       0.169       0.685: 100%|█████\n    20/100     5.72G        2.19        2.38       0.136       0.513: 100%|█████\n    21/100     5.72G        2.21         2.3       0.139        0.55: 100%|█████\n    22/100     5.72G        2.16        2.35       0.109       0.493: 100%|█████\n    23/100     5.72G        2.16        2.46       0.113       0.517: 100%|█████\n    24/100     5.72G        2.18        2.32       0.149       0.666: 100%|█████\n    25/100     5.72G        2.13        2.22       0.182       0.709: 100%|█████\n    26/100     5.72G        2.15        2.24       0.142       0.576: 100%|█████\n    27/100     5.72G        2.14        2.14       0.222       0.722: 100%|█████\n    28/100     5.72G        2.13        2.21       0.182       0.666: 100%|█████\n    29/100     5.72G        2.14         2.5       0.209       0.712: 100%|█████\n    30/100     5.72G        2.12        2.22       0.189       0.702: 100%|█████\n    31/100     5.72G        2.12        2.18       0.182       0.709: 100%|█████\n    32/100     5.72G        2.12        2.17       0.212       0.778: 100%|█████\n    33/100     5.72G        2.12        2.13       0.238       0.738: 100%|█████\n    34/100     5.72G        2.12        2.31       0.152       0.583: 100%|█████\n    35/100     5.72G        2.12        2.23       0.156       0.623: 100%|█████\n    36/100     5.72G        2.12        2.23       0.156       0.636: 100%|█████\n    37/100     5.72G        2.12        2.22       0.149       0.646: 100%|█████\n    38/100     5.72G        2.11        2.34       0.146       0.543: 100%|█████\n    39/100     5.72G         2.1        2.41       0.139        0.53: 100%|█████\n    40/100     5.72G        2.11        2.33       0.149       0.579: 100%|█████\n    41/100     5.72G        2.09         2.4       0.152       0.526: 100%|█████\n    42/100     5.72G         2.1        2.28       0.156       0.603: 100%|█████\n    43/100     5.72G        2.09         2.3       0.149        0.55: 100%|█████\n    44/100     5.72G        2.08        2.32       0.149       0.563: 100%|█████\n    45/100     5.72G        2.09         2.3       0.156       0.576: 100%|█████\n    46/100     5.72G        2.08        2.44       0.132       0.503: 100%|█████\n    47/100     5.72G         2.1        2.38       0.149        0.51: 100%|█████\n    48/100     5.72G        2.07        2.16       0.209       0.709: 100%|█████\n    49/100     5.72G        2.05        2.16       0.209       0.715: 100%|█████\n    50/100     5.72G        2.09        2.23       0.182       0.629: 100%|█████\n    51/100     5.72G        2.08        2.34       0.146       0.563: 100%|█████\n    52/100     5.72G        2.08        2.25       0.149       0.593: 100%|█████\n    53/100     5.72G        2.09         2.2       0.152       0.613: 100%|█████\n    54/100     5.72G        2.03        2.32       0.126       0.563: 100%|█████\n    55/100     5.72G        2.08        2.18       0.182       0.662: 100%|█████\n    56/100     5.72G        2.06        2.17       0.209       0.715: 100%|█████\n    57/100     5.72G        2.03        2.21       0.169       0.623: 100%|█████\n    58/100     5.72G        2.06        2.21       0.162       0.613: 100%|█████\n    59/100     5.72G        2.08        2.18       0.215       0.728: 100%|█████\n    60/100     5.72G        2.03        2.11       0.235       0.742: 100%|█████\n    61/100     5.72G        2.03        2.31       0.152       0.593: 100%|█████\n    62/100     5.72G        2.06        2.31       0.159       0.579: 100%|█████\n    63/100     5.72G        2.06        2.17       0.212       0.719: 100%|█████\n    64/100     5.72G        2.05        2.22       0.199       0.642: 100%|█████\n    65/100     5.72G        2.06        2.23       0.166       0.616: 100%|█████\n    66/100     5.72G        2.03         2.2       0.169       0.679: 100%|█████\n    67/100     5.72G        2.04        2.12       0.205       0.758: 100%|█████\n    68/100     5.72G        2.02        2.16       0.192       0.705: 100%|█████\n    69/100     5.72G        2.03         2.2       0.172       0.646: 100%|█████\n    70/100     5.72G        2.02        2.15       0.209       0.695: 100%|█████\n    71/100     5.72G        2.05        2.16       0.195       0.675: 100%|█████\n    72/100     5.72G        2.02        2.12       0.222       0.755: 100%|█████\n    73/100     5.72G        2.01         2.1       0.232       0.772: 100%|█████\n    74/100     5.72G        2.01        2.15       0.192       0.715: 100%|█████\n    75/100     5.72G        2.01        2.18       0.185       0.669: 100%|█████\n    76/100     5.72G        2.02        2.09       0.222       0.758: 100%|█████\n    77/100     5.72G        2.01        2.06       0.245       0.765: 100%|█████\n    78/100     5.72G        2.01        2.16       0.215       0.735: 100%|█████\n    79/100     5.72G           2        2.41       0.215       0.712: 100%|█████\n    80/100     5.72G           2        2.08       0.245       0.765: 100%|█████\n    81/100     5.72G        2.03        2.15       0.222       0.728: 100%|█████\n    82/100     5.72G        2.01        2.08       0.255       0.752: 100%|█████\n    83/100     5.72G        2.01        2.09       0.285       0.752: 100%|█████\n    84/100     5.72G           2        2.04       0.252       0.752: 100%|█████\n    85/100     5.72G        1.99        2.12       0.252       0.745: 100%|█████\n    86/100     5.72G        1.98        2.11       0.255       0.762: 100%|█████\n    87/100     5.72G        1.97        2.06       0.248       0.775: 100%|█████\n    88/100     5.72G        1.98        2.08       0.238       0.785: 100%|█████\n    89/100     5.72G        1.97        2.11       0.238       0.755: 100%|█████\n    90/100     5.72G        1.97        2.12       0.225       0.772: 100%|█████\n    91/100     5.72G        1.96        2.14       0.255       0.778: 100%|█████\n    92/100     5.72G        1.95         2.1       0.285       0.768: 100%|█████\n    93/100     5.72G        1.96        2.04       0.275       0.781: 100%|█████\n    94/100     5.72G        1.92        2.04       0.288       0.778: 100%|█████\n    95/100     5.72G         1.9        2.04       0.288       0.778: 100%|█████\n    96/100     5.72G        1.96        2.07       0.262       0.791: 100%|█████\n    97/100     5.72G        1.95        2.05       0.301       0.762: 100%|█████\n    98/100     5.72G        1.95        2.06       0.311       0.778: 100%|█████\n    99/100     5.72G         1.9        2.02       0.288       0.798: 100%|█████\n   100/100     5.72G        1.92        2.05       0.288       0.795: 100%|█████\n\nTraining complete (0.207 hours)\nResults saved to \u001b[1mruns/train-cls/exp5\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp5/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp5/weights/best.pt --data output\nExport:          python export.py --weights runs/train-cls/exp5/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp5/weights/best.pt')\nVisualize:       https://netron.app\n\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f160f0e4b00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f160f0e4b00>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\nAttributeError: 'NoneType' object has no attribute 'python_exit_status'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Validate on validation dataset for all 5 models (n, s, m, l, x)","metadata":{"id":"HHUFGeLbGd98"}},{"cell_type":"code","source":"!python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data ./output/","metadata":{"id":"DIV7ydyKGZFL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46f6675d-f8b2-42c0-b0a9-98cf2e984ad1","execution":{"iopub.status.busy":"2022-11-11T06:41:18.339543Z","iopub.execute_input":"2022-11-11T06:41:18.340010Z","iopub.status.idle":"2022-11-11T06:41:27.935023Z","shell.execute_reply.started":"2022-11-11T06:41:18.339955Z","shell.execute_reply":"2022-11-11T06:41:27.933817Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=./output/, weights=['runs/train-cls/exp/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nvalidating: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]                       \n                   Class      Images    top1_acc    top5_acc\n                     all         302       0.444       0.868\n                  fabric          31       0.194       0.806\n                 foliage          31       0.677       0.839\n                   glass          30       0.467       0.933\n                 leather          30       0.367         0.9\n                   metal          30       0.333       0.867\n                   paper          30       0.433       0.767\n                 plastic          30         0.3         0.9\n                   stone          30       0.567       0.867\n                   water          30       0.667       0.967\n                    wood          30       0.433       0.833\nSpeed: 0.1ms pre-process, 0.6ms inference, 0.0ms post-process per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/val-cls/exp\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!python classify/val.py --weights runs/train-cls/exp2/weights/best.pt --data ./output/","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:41:29.189063Z","iopub.execute_input":"2022-11-11T06:41:29.190090Z","iopub.status.idle":"2022-11-11T06:41:39.049149Z","shell.execute_reply.started":"2022-11-11T06:41:29.190049Z","shell.execute_reply":"2022-11-11T06:41:39.047929Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=./output/, weights=['runs/train-cls/exp2/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nvalidating: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]                       \n                   Class      Images    top1_acc    top5_acc\n                     all         302       0.397       0.834\n                  fabric          31       0.129       0.677\n                 foliage          31        0.71       0.903\n                   glass          30       0.267       0.967\n                 leather          30         0.3       0.833\n                   metal          30       0.333       0.867\n                   paper          30       0.233       0.767\n                 plastic          30         0.3         0.8\n                   stone          30         0.7         0.8\n                   water          30         0.7       0.933\n                    wood          30         0.3         0.8\nSpeed: 0.1ms pre-process, 0.8ms inference, 0.0ms post-process per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/val-cls/exp2\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!python classify/val.py --weights runs/train-cls/exp3/weights/best.pt --data ./output/","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:41:44.659230Z","iopub.execute_input":"2022-11-11T06:41:44.659615Z","iopub.status.idle":"2022-11-11T06:41:54.595642Z","shell.execute_reply.started":"2022-11-11T06:41:44.659583Z","shell.execute_reply":"2022-11-11T06:41:54.594427Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=./output/, weights=['runs/train-cls/exp3/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nvalidating: 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]                       \n                   Class      Images    top1_acc    top5_acc\n                     all         302       0.368       0.831\n                  fabric          31      0.0323       0.871\n                 foliage          31       0.645       0.839\n                   glass          30         0.3       0.733\n                 leather          30       0.167         0.8\n                   metal          30       0.367       0.867\n                   paper          30       0.233       0.767\n                 plastic          30       0.333       0.933\n                   stone          30         0.7       0.867\n                   water          30         0.7         0.8\n                    wood          30         0.2       0.833\nSpeed: 0.1ms pre-process, 1.5ms inference, 0.0ms post-process per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/val-cls/exp3\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!python classify/val.py --weights runs/train-cls/exp4/weights/best.pt --data ./output/","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:42:02.941822Z","iopub.execute_input":"2022-11-11T06:42:02.942238Z","iopub.status.idle":"2022-11-11T06:42:13.483312Z","shell.execute_reply.started":"2022-11-11T06:42:02.942202Z","shell.execute_reply":"2022-11-11T06:42:13.482142Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=./output/, weights=['runs/train-cls/exp4/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nvalidating: 100%|██████████| 3/3 [00:01<00:00,  1.72it/s]                       \n                   Class      Images    top1_acc    top5_acc\n                     all         302       0.364       0.811\n                  fabric          31      0.0968       0.935\n                 foliage          31       0.677       0.742\n                   glass          30         0.3         0.8\n                 leather          30       0.167       0.667\n                   metal          30         0.2       0.833\n                   paper          30         0.1       0.933\n                 plastic          30         0.5       0.933\n                   stone          30         0.6       0.733\n                   water          30       0.733         0.9\n                    wood          30       0.267       0.633\nSpeed: 0.1ms pre-process, 1.7ms inference, 0.0ms post-process per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/val-cls/exp4\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!python classify/val.py --weights runs/train-cls/exp5/weights/best.pt --data ./output/","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:42:13.485448Z","iopub.execute_input":"2022-11-11T06:42:13.485853Z","iopub.status.idle":"2022-11-11T06:42:23.994993Z","shell.execute_reply.started":"2022-11-11T06:42:13.485812Z","shell.execute_reply":"2022-11-11T06:42:23.993823Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=./output/, weights=['runs/train-cls/exp5/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nvalidating: 100%|██████████| 3/3 [00:01<00:00,  1.51it/s]                       \n                   Class      Images    top1_acc    top5_acc\n                     all         302       0.311       0.778\n                  fabric          31       0.161       0.548\n                 foliage          31        0.71       0.806\n                   glass          30         0.1       0.733\n                 leather          30         0.5       0.833\n                   metal          30         0.2       0.967\n                   paper          30       0.133       0.733\n                 plastic          30       0.467       0.933\n                   stone          30       0.267         0.8\n                   water          30       0.433       0.867\n                    wood          30       0.133       0.567\nSpeed: 0.1ms pre-process, 3.0ms inference, 0.1ms post-process per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/val-cls/exp5\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Predict on test dataset for all 5 models (n, s, m, l, x)","metadata":{}},{"cell_type":"code","source":"materials = ['fabric', 'foliage', 'glass', 'leather', 'metal', 'paper', 'plastic', 'stone', 'water', 'wood']\nfor x in materials:\n    !python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source ./output/test/{x}","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:42:34.381007Z","iopub.execute_input":"2022-11-11T06:42:34.381707Z","iopub.status.idle":"2022-11-11T06:44:03.642894Z","shell.execute_reply.started":"2022-11-11T06:42:34.381664Z","shell.execute_reply":"2022-11-11T06:44:03.641691Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/fabric, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/fabric/IMG_1909.JPG: 224x224 paper 0.40, leather 0.26, fabric 0.16, metal 0.10, wood 0.03, 6.7ms\nimage 2/3 /kaggle/working/yolov5/output/test/fabric/Image1.jpeg: 224x224 paper 0.46, metal 0.16, leather 0.12, plastic 0.07, fabric 0.06, 3.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/fabric/Image2.jpeg: 224x224 wood 0.36, stone 0.31, leather 0.21, fabric 0.05, metal 0.03, 3.5ms\nSpeed: 0.3ms pre-process, 4.6ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp\u001b[0m\n3 labels saved to runs/predict-cls/exp/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/foliage, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/foliage/IMG_1916.JPG: 224x224 foliage 0.75, water 0.20, glass 0.03, metal 0.01, wood 0.00, 2.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/foliage/Image1.jpeg: 224x224 foliage 0.44, water 0.30, glass 0.15, fabric 0.03, metal 0.02, 3.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/foliage/Image2.jpeg: 224x224 water 0.63, foliage 0.23, fabric 0.04, stone 0.03, glass 0.02, 3.5ms\nSpeed: 0.3ms pre-process, 3.3ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp2\u001b[0m\n3 labels saved to runs/predict-cls/exp2/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/glass, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/glass/IMG_1914.JPG: 224x224 metal 0.37, paper 0.25, glass 0.16, leather 0.09, plastic 0.05, 3.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/glass/Image1.jpeg: 224x224 stone 0.42, wood 0.14, fabric 0.14, leather 0.12, water 0.06, 3.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/glass/Image2.jpeg: 224x224 glass 0.22, stone 0.16, metal 0.13, wood 0.13, water 0.12, 3.5ms\nSpeed: 0.3ms pre-process, 3.5ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp3\u001b[0m\n3 labels saved to runs/predict-cls/exp3/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/leather, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/leather/IMG_1907.JPG: 224x224 leather 0.35, fabric 0.27, wood 0.22, stone 0.11, metal 0.02, 3.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/leather/Image1.jpeg: 224x224 leather 0.26, fabric 0.20, paper 0.16, stone 0.11, metal 0.11, 3.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/leather/Image2.jpeg: 224x224 leather 0.69, stone 0.17, fabric 0.07, paper 0.05, wood 0.01, 3.4ms\nSpeed: 0.3ms pre-process, 3.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp4\u001b[0m\n3 labels saved to runs/predict-cls/exp4/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/metal, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/metal/IMG_1912.JPG: 224x224 fabric 0.34, stone 0.18, leather 0.11, metal 0.10, wood 0.08, 3.3ms\nimage 2/3 /kaggle/working/yolov5/output/test/metal/Image1.jpeg: 224x224 metal 0.37, leather 0.23, paper 0.22, fabric 0.08, wood 0.03, 3.7ms\nimage 3/3 /kaggle/working/yolov5/output/test/metal/Image2.jpeg: 224x224 stone 0.21, plastic 0.18, wood 0.16, water 0.14, metal 0.11, 3.5ms\nSpeed: 0.3ms pre-process, 3.5ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp5\u001b[0m\n3 labels saved to runs/predict-cls/exp5/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/paper, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/paper/IMG_1911.JPG: 224x224 water 0.36, plastic 0.28, paper 0.10, metal 0.07, glass 0.06, 3.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/paper/Image1.jpeg: 224x224 metal 0.35, leather 0.28, paper 0.21, fabric 0.07, wood 0.05, 3.4ms\nimage 3/3 /kaggle/working/yolov5/output/test/paper/Image2.jpeg: 224x224 water 0.59, paper 0.28, glass 0.04, wood 0.03, foliage 0.02, 3.4ms\nSpeed: 0.3ms pre-process, 3.5ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp6\u001b[0m\n3 labels saved to runs/predict-cls/exp6/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/plastic, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/plastic/IMG_1913.JPG: 224x224 paper 0.30, plastic 0.21, metal 0.21, glass 0.16, fabric 0.06, 3.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/plastic/Image1.jpeg: 224x224 plastic 0.67, paper 0.14, metal 0.13, leather 0.02, water 0.01, 3.6ms\nimage 3/3 /kaggle/working/yolov5/output/test/plastic/Image2.jpeg: 224x224 paper 0.19, glass 0.15, water 0.15, fabric 0.14, plastic 0.13, 3.9ms\nSpeed: 0.3ms pre-process, 3.7ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp7\u001b[0m\n3 labels saved to runs/predict-cls/exp7/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/stone, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/stone/IMG_1918.JPG: 224x224 stone 0.30, fabric 0.28, water 0.15, foliage 0.11, paper 0.05, 4.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/stone/Image1.jpeg: 224x224 fabric 0.41, stone 0.34, leather 0.14, wood 0.07, foliage 0.01, 3.6ms\nimage 3/3 /kaggle/working/yolov5/output/test/stone/Image2.jpeg: 224x224 water 0.55, stone 0.20, plastic 0.07, glass 0.04, leather 0.04, 3.5ms\nSpeed: 0.3ms pre-process, 3.9ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp8\u001b[0m\n3 labels saved to runs/predict-cls/exp8/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/water, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/water/IMG_1915.JPG: 224x224 leather 0.31, wood 0.19, stone 0.18, metal 0.12, fabric 0.09, 3.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/water/Image1.jpeg: 224x224 plastic 0.44, water 0.24, paper 0.11, metal 0.08, glass 0.03, 3.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/water/Image2.jpeg: 224x224 metal 0.43, plastic 0.21, paper 0.13, leather 0.13, glass 0.03, 3.4ms\nSpeed: 0.3ms pre-process, 3.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp9\u001b[0m\n3 labels saved to runs/predict-cls/exp9/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp/weights/best.pt'], source=./output/test/wood, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 1221274 parameters, 0 gradients, 2.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/wood/IMG_1906.JPG: 224x224 stone 0.25, leather 0.24, wood 0.20, fabric 0.20, foliage 0.04, 3.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/wood/Image1.jpeg: 224x224 wood 0.63, stone 0.33, paper 0.02, water 0.01, fabric 0.01, 4.1ms\nimage 3/3 /kaggle/working/yolov5/output/test/wood/Image2.jpeg: 224x224 wood 0.37, leather 0.25, metal 0.19, fabric 0.12, stone 0.04, 3.5ms\nSpeed: 0.3ms pre-process, 3.7ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp10\u001b[0m\n3 labels saved to runs/predict-cls/exp10/labels\n","output_type":"stream"}]},{"cell_type":"code","source":"for x in materials:\n    !python classify/predict.py --weights runs/train-cls/exp2/weights/best.pt --source ./output/test/{x}","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:44:03.645385Z","iopub.execute_input":"2022-11-11T06:44:03.646194Z","iopub.status.idle":"2022-11-11T06:45:34.067382Z","shell.execute_reply.started":"2022-11-11T06:44:03.646127Z","shell.execute_reply":"2022-11-11T06:45:34.066154Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/fabric, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/fabric/IMG_1909.JPG: 224x224 stone 0.27, paper 0.17, leather 0.16, wood 0.12, fabric 0.07, 3.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/fabric/Image1.jpeg: 224x224 stone 0.29, leather 0.24, fabric 0.12, paper 0.12, plastic 0.08, 3.6ms\nimage 3/3 /kaggle/working/yolov5/output/test/fabric/Image2.jpeg: 224x224 stone 0.44, leather 0.24, fabric 0.17, wood 0.09, paper 0.02, 3.6ms\nSpeed: 0.3ms pre-process, 3.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp11\u001b[0m\n3 labels saved to runs/predict-cls/exp11/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/foliage, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/foliage/IMG_1916.JPG: 224x224 foliage 0.88, glass 0.05, water 0.03, stone 0.01, fabric 0.01, 3.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/foliage/Image1.jpeg: 224x224 foliage 0.46, fabric 0.22, glass 0.17, water 0.09, stone 0.02, 3.7ms\nimage 3/3 /kaggle/working/yolov5/output/test/foliage/Image2.jpeg: 224x224 fabric 0.47, foliage 0.30, glass 0.06, water 0.06, wood 0.04, 3.6ms\nSpeed: 0.3ms pre-process, 3.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp12\u001b[0m\n3 labels saved to runs/predict-cls/exp12/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/glass, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/glass/IMG_1914.JPG: 224x224 paper 0.18, metal 0.16, leather 0.14, stone 0.13, wood 0.12, 3.7ms\nimage 2/3 /kaggle/working/yolov5/output/test/glass/Image1.jpeg: 224x224 stone 0.46, wood 0.15, leather 0.13, fabric 0.11, water 0.05, 3.6ms\nimage 3/3 /kaggle/working/yolov5/output/test/glass/Image2.jpeg: 224x224 stone 0.34, wood 0.19, leather 0.11, water 0.09, metal 0.06, 4.1ms\nSpeed: 0.3ms pre-process, 3.8ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp13\u001b[0m\n3 labels saved to runs/predict-cls/exp13/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/leather, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/leather/IMG_1907.JPG: 224x224 stone 0.32, wood 0.32, fabric 0.15, leather 0.08, water 0.05, 5.4ms\nimage 2/3 /kaggle/working/yolov5/output/test/leather/Image1.jpeg: 224x224 stone 0.39, leather 0.30, fabric 0.11, paper 0.08, foliage 0.04, 3.6ms\nimage 3/3 /kaggle/working/yolov5/output/test/leather/Image2.jpeg: 224x224 stone 0.60, leather 0.18, fabric 0.17, wood 0.02, paper 0.02, 7.3ms\nSpeed: 0.4ms pre-process, 5.4ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp14\u001b[0m\n3 labels saved to runs/predict-cls/exp14/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/metal, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/metal/IMG_1912.JPG: 224x224 stone 0.26, leather 0.15, wood 0.12, fabric 0.11, water 0.11, 3.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/metal/Image1.jpeg: 224x224 stone 0.28, wood 0.15, leather 0.12, metal 0.11, paper 0.10, 5.9ms\nimage 3/3 /kaggle/working/yolov5/output/test/metal/Image2.jpeg: 224x224 stone 0.40, water 0.14, wood 0.12, fabric 0.09, leather 0.08, 4.7ms\nSpeed: 0.4ms pre-process, 4.7ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp15\u001b[0m\n3 labels saved to runs/predict-cls/exp15/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/paper, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/paper/IMG_1911.JPG: 224x224 plastic 0.36, paper 0.22, metal 0.13, water 0.12, glass 0.06, 3.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/paper/Image1.jpeg: 224x224 metal 0.58, wood 0.11, glass 0.09, leather 0.08, paper 0.05, 3.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/paper/Image2.jpeg: 224x224 stone 0.36, wood 0.23, water 0.17, metal 0.08, paper 0.06, 4.0ms\nSpeed: 0.3ms pre-process, 3.7ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp16\u001b[0m\n3 labels saved to runs/predict-cls/exp16/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/plastic, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/plastic/IMG_1913.JPG: 224x224 paper 0.32, plastic 0.22, metal 0.13, glass 0.11, water 0.07, 3.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/plastic/Image1.jpeg: 224x224 plastic 0.37, paper 0.19, metal 0.12, water 0.11, glass 0.10, 4.1ms\nimage 3/3 /kaggle/working/yolov5/output/test/plastic/Image2.jpeg: 224x224 metal 0.32, glass 0.18, paper 0.14, leather 0.07, plastic 0.06, 3.8ms\nSpeed: 0.3ms pre-process, 3.9ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp17\u001b[0m\n3 labels saved to runs/predict-cls/exp17/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/stone, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/stone/IMG_1918.JPG: 224x224 stone 0.44, fabric 0.17, wood 0.16, foliage 0.06, leather 0.04, 3.1ms\nimage 2/3 /kaggle/working/yolov5/output/test/stone/Image1.jpeg: 224x224 stone 0.60, leather 0.27, fabric 0.07, wood 0.02, water 0.02, 3.6ms\nimage 3/3 /kaggle/working/yolov5/output/test/stone/Image2.jpeg: 224x224 water 0.43, plastic 0.22, fabric 0.08, metal 0.07, stone 0.06, 4.1ms\nSpeed: 0.3ms pre-process, 3.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp18\u001b[0m\n3 labels saved to runs/predict-cls/exp18/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/water, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/water/IMG_1915.JPG: 224x224 wood 0.24, leather 0.16, metal 0.15, stone 0.15, fabric 0.11, 3.3ms\nimage 2/3 /kaggle/working/yolov5/output/test/water/Image1.jpeg: 224x224 plastic 0.33, water 0.18, paper 0.17, metal 0.13, fabric 0.05, 4.0ms\nimage 3/3 /kaggle/working/yolov5/output/test/water/Image2.jpeg: 224x224 paper 0.20, plastic 0.19, metal 0.14, leather 0.11, stone 0.10, 3.5ms\nSpeed: 0.3ms pre-process, 3.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp19\u001b[0m\n3 labels saved to runs/predict-cls/exp19/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp2/weights/best.pt'], source=./output/test/wood, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 117 layers, 4179498 parameters, 0 gradients, 10.4 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/wood/IMG_1906.JPG: 224x224 fabric 0.50, stone 0.25, wood 0.20, leather 0.02, foliage 0.02, 3.7ms\nimage 2/3 /kaggle/working/yolov5/output/test/wood/Image1.jpeg: 224x224 wood 0.36, stone 0.30, water 0.20, fabric 0.06, leather 0.03, 3.6ms\nimage 3/3 /kaggle/working/yolov5/output/test/wood/Image2.jpeg: 224x224 wood 0.61, stone 0.23, fabric 0.08, leather 0.02, metal 0.02, 5.2ms\nSpeed: 0.3ms pre-process, 4.2ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp20\u001b[0m\n3 labels saved to runs/predict-cls/exp20/labels\n","output_type":"stream"}]},{"cell_type":"code","source":"for x in materials:\n    !python classify/predict.py --weights runs/train-cls/exp3/weights/best.pt --source ./output/test/{x}","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:45:34.070245Z","iopub.execute_input":"2022-11-11T06:45:34.071495Z","iopub.status.idle":"2022-11-11T06:47:04.736939Z","shell.execute_reply.started":"2022-11-11T06:45:34.071448Z","shell.execute_reply":"2022-11-11T06:47:04.735712Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/fabric, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/fabric/IMG_1909.JPG: 224x224 stone 0.21, paper 0.18, leather 0.16, metal 0.09, wood 0.08, 5.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/fabric/Image1.jpeg: 224x224 paper 0.25, plastic 0.15, metal 0.12, leather 0.11, stone 0.10, 8.4ms\nimage 3/3 /kaggle/working/yolov5/output/test/fabric/Image2.jpeg: 224x224 stone 0.47, wood 0.21, leather 0.16, fabric 0.07, water 0.04, 5.8ms\nSpeed: 0.3ms pre-process, 6.7ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp21\u001b[0m\n3 labels saved to runs/predict-cls/exp21/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/foliage, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/foliage/IMG_1916.JPG: 224x224 foliage 0.88, water 0.04, glass 0.03, plastic 0.02, fabric 0.01, 6.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/foliage/Image1.jpeg: 224x224 foliage 0.83, glass 0.06, water 0.04, plastic 0.03, fabric 0.01, 5.9ms\nimage 3/3 /kaggle/working/yolov5/output/test/foliage/Image2.jpeg: 224x224 foliage 0.80, water 0.05, plastic 0.05, fabric 0.03, glass 0.03, 6.1ms\nSpeed: 0.3ms pre-process, 6.2ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp22\u001b[0m\n3 labels saved to runs/predict-cls/exp22/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/glass, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/glass/IMG_1914.JPG: 224x224 paper 0.20, leather 0.14, metal 0.13, stone 0.13, glass 0.09, 5.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/glass/Image1.jpeg: 224x224 stone 0.40, wood 0.17, leather 0.15, fabric 0.08, water 0.06, 5.9ms\nimage 3/3 /kaggle/working/yolov5/output/test/glass/Image2.jpeg: 224x224 stone 0.27, leather 0.20, wood 0.11, water 0.11, paper 0.10, 5.8ms\nSpeed: 0.3ms pre-process, 5.9ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp23\u001b[0m\n3 labels saved to runs/predict-cls/exp23/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/leather, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/leather/IMG_1907.JPG: 224x224 wood 0.51, stone 0.27, fabric 0.11, metal 0.06, leather 0.02, 5.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/leather/Image1.jpeg: 224x224 stone 0.24, leather 0.21, paper 0.13, water 0.10, wood 0.08, 5.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/leather/Image2.jpeg: 224x224 stone 0.45, wood 0.26, leather 0.14, fabric 0.10, water 0.04, 5.8ms\nSpeed: 0.3ms pre-process, 5.8ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp24\u001b[0m\n3 labels saved to runs/predict-cls/exp24/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/metal, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/metal/IMG_1912.JPG: 224x224 stone 0.26, leather 0.14, paper 0.12, water 0.11, wood 0.10, 9.3ms\nimage 2/3 /kaggle/working/yolov5/output/test/metal/Image1.jpeg: 224x224 paper 0.21, metal 0.21, stone 0.16, glass 0.09, fabric 0.09, 5.9ms\nimage 3/3 /kaggle/working/yolov5/output/test/metal/Image2.jpeg: 224x224 water 0.32, stone 0.21, leather 0.13, wood 0.10, fabric 0.08, 6.1ms\nSpeed: 0.3ms pre-process, 7.1ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp25\u001b[0m\n3 labels saved to runs/predict-cls/exp25/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/paper, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/paper/IMG_1911.JPG: 224x224 plastic 0.36, paper 0.23, leather 0.09, metal 0.08, water 0.08, 6.0ms\nimage 2/3 /kaggle/working/yolov5/output/test/paper/Image1.jpeg: 224x224 stone 0.32, leather 0.14, paper 0.13, wood 0.08, water 0.08, 5.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/paper/Image2.jpeg: 224x224 water 0.66, stone 0.08, leather 0.07, wood 0.05, fabric 0.04, 5.9ms\nSpeed: 0.3ms pre-process, 5.9ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp26\u001b[0m\n3 labels saved to runs/predict-cls/exp26/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/plastic, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/plastic/IMG_1913.JPG: 224x224 paper 0.25, plastic 0.24, metal 0.15, glass 0.09, water 0.08, 5.8ms\nimage 2/3 /kaggle/working/yolov5/output/test/plastic/Image1.jpeg: 224x224 plastic 0.27, paper 0.23, metal 0.13, water 0.12, glass 0.09, 5.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/plastic/Image2.jpeg: 224x224 plastic 0.24, paper 0.19, metal 0.13, fabric 0.12, water 0.09, 5.9ms\nSpeed: 0.3ms pre-process, 5.8ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp27\u001b[0m\n3 labels saved to runs/predict-cls/exp27/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/stone, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/stone/IMG_1918.JPG: 224x224 stone 0.20, glass 0.14, paper 0.13, fabric 0.13, metal 0.13, 5.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/stone/Image1.jpeg: 224x224 stone 0.53, wood 0.16, leather 0.12, fabric 0.07, water 0.05, 5.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/stone/Image2.jpeg: 224x224 plastic 0.25, paper 0.21, water 0.17, metal 0.10, leather 0.08, 5.8ms\nSpeed: 0.3ms pre-process, 5.7ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp28\u001b[0m\n3 labels saved to runs/predict-cls/exp28/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/water, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/water/IMG_1915.JPG: 224x224 wood 0.31, stone 0.22, metal 0.12, leather 0.11, fabric 0.09, 5.8ms\nimage 2/3 /kaggle/working/yolov5/output/test/water/Image1.jpeg: 224x224 plastic 0.33, paper 0.24, metal 0.10, water 0.10, leather 0.07, 6.1ms\nimage 3/3 /kaggle/working/yolov5/output/test/water/Image2.jpeg: 224x224 paper 0.24, leather 0.14, metal 0.12, plastic 0.11, stone 0.11, 6.1ms\nSpeed: 0.3ms pre-process, 6.0ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp29\u001b[0m\n3 labels saved to runs/predict-cls/exp29/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp3/weights/best.pt'], source=./output/test/wood, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 166 layers, 11679002 parameters, 0 gradients, 30.6 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/wood/IMG_1906.JPG: 224x224 stone 0.39, wood 0.35, fabric 0.17, leather 0.06, foliage 0.01, 5.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/wood/Image1.jpeg: 224x224 stone 0.31, water 0.24, wood 0.19, leather 0.17, fabric 0.06, 5.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/wood/Image2.jpeg: 224x224 wood 0.52, metal 0.12, leather 0.12, stone 0.11, fabric 0.06, 5.9ms\nSpeed: 0.3ms pre-process, 5.9ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp30\u001b[0m\n3 labels saved to runs/predict-cls/exp30/labels\n","output_type":"stream"}]},{"cell_type":"code","source":"for x in materials:\n    !python classify/predict.py --weights runs/train-cls/exp4/weights/best.pt --source ./output/test/{x}","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:47:04.741103Z","iopub.execute_input":"2022-11-11T06:47:04.741433Z","iopub.status.idle":"2022-11-11T06:48:37.149428Z","shell.execute_reply.started":"2022-11-11T06:47:04.741403Z","shell.execute_reply":"2022-11-11T06:48:37.148213Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/fabric, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/fabric/IMG_1909.JPG: 224x224 stone 0.31, water 0.15, wood 0.10, fabric 0.09, metal 0.09, 13.6ms\nimage 2/3 /kaggle/working/yolov5/output/test/fabric/Image1.jpeg: 224x224 water 0.28, stone 0.13, paper 0.12, fabric 0.11, glass 0.10, 6.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/fabric/Image2.jpeg: 224x224 stone 0.45, wood 0.18, leather 0.10, water 0.08, fabric 0.06, 6.5ms\nSpeed: 0.4ms pre-process, 8.9ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp31\u001b[0m\n3 labels saved to runs/predict-cls/exp31/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/foliage, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/foliage/IMG_1916.JPG: 224x224 foliage 0.83, water 0.06, plastic 0.05, glass 0.02, fabric 0.01, 6.3ms\nimage 2/3 /kaggle/working/yolov5/output/test/foliage/Image1.jpeg: 224x224 foliage 0.71, water 0.08, plastic 0.07, glass 0.06, fabric 0.02, 6.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/foliage/Image2.jpeg: 224x224 foliage 0.73, plastic 0.08, water 0.07, fabric 0.05, glass 0.02, 6.5ms\nSpeed: 0.3ms pre-process, 6.5ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp32\u001b[0m\n3 labels saved to runs/predict-cls/exp32/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/glass, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/glass/IMG_1914.JPG: 224x224 stone 0.26, wood 0.12, water 0.12, leather 0.11, fabric 0.10, 6.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/glass/Image1.jpeg: 224x224 stone 0.43, wood 0.17, leather 0.09, water 0.09, fabric 0.07, 6.6ms\nimage 3/3 /kaggle/working/yolov5/output/test/glass/Image2.jpeg: 224x224 stone 0.40, water 0.13, wood 0.13, leather 0.10, fabric 0.06, 6.5ms\nSpeed: 0.3ms pre-process, 6.7ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp33\u001b[0m\n3 labels saved to runs/predict-cls/exp33/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/leather, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/leather/IMG_1907.JPG: 224x224 wood 0.37, stone 0.26, leather 0.09, metal 0.08, fabric 0.08, 6.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/leather/Image1.jpeg: 224x224 stone 0.34, water 0.32, leather 0.08, wood 0.06, fabric 0.05, 7.0ms\nimage 3/3 /kaggle/working/yolov5/output/test/leather/Image2.jpeg: 224x224 stone 0.46, wood 0.24, leather 0.09, fabric 0.05, water 0.05, 6.6ms\nSpeed: 0.3ms pre-process, 6.7ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp34\u001b[0m\n3 labels saved to runs/predict-cls/exp34/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/metal, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/metal/IMG_1912.JPG: 224x224 water 0.18, stone 0.18, metal 0.12, paper 0.11, fabric 0.10, 6.3ms\nimage 2/3 /kaggle/working/yolov5/output/test/metal/Image1.jpeg: 224x224 stone 0.21, metal 0.17, fabric 0.13, paper 0.12, wood 0.10, 6.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/metal/Image2.jpeg: 224x224 water 0.25, paper 0.13, fabric 0.11, plastic 0.10, metal 0.10, 6.7ms\nSpeed: 0.3ms pre-process, 6.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp35\u001b[0m\n3 labels saved to runs/predict-cls/exp35/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/paper, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/paper/IMG_1911.JPG: 224x224 plastic 0.32, leather 0.13, paper 0.12, fabric 0.12, water 0.12, 6.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/paper/Image1.jpeg: 224x224 water 0.23, paper 0.14, fabric 0.12, glass 0.12, metal 0.11, 6.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/paper/Image2.jpeg: 224x224 water 0.37, stone 0.27, leather 0.08, paper 0.06, fabric 0.06, 6.5ms\nSpeed: 0.3ms pre-process, 6.5ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp36\u001b[0m\n3 labels saved to runs/predict-cls/exp36/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/plastic, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/plastic/IMG_1913.JPG: 224x224 plastic 0.21, paper 0.18, fabric 0.14, glass 0.14, metal 0.12, 6.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/plastic/Image1.jpeg: 224x224 water 0.67, plastic 0.07, paper 0.06, glass 0.06, fabric 0.05, 6.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/plastic/Image2.jpeg: 224x224 plastic 0.19, paper 0.16, fabric 0.14, water 0.11, metal 0.11, 6.4ms\nSpeed: 0.3ms pre-process, 6.5ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp37\u001b[0m\n3 labels saved to runs/predict-cls/exp37/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/stone, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/stone/IMG_1918.JPG: 224x224 stone 0.38, water 0.14, wood 0.11, leather 0.09, fabric 0.08, 6.0ms\nimage 2/3 /kaggle/working/yolov5/output/test/stone/Image1.jpeg: 224x224 stone 0.38, wood 0.31, leather 0.08, fabric 0.07, metal 0.06, 7.0ms\nimage 3/3 /kaggle/working/yolov5/output/test/stone/Image2.jpeg: 224x224 plastic 0.24, water 0.19, paper 0.14, fabric 0.11, leather 0.11, 16.2ms\nSpeed: 0.3ms pre-process, 9.7ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp38\u001b[0m\n3 labels saved to runs/predict-cls/exp38/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/water, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/water/IMG_1915.JPG: 224x224 stone 0.31, wood 0.20, metal 0.11, fabric 0.09, leather 0.09, 6.1ms\nimage 2/3 /kaggle/working/yolov5/output/test/water/Image1.jpeg: 224x224 plastic 0.29, water 0.14, paper 0.13, fabric 0.12, leather 0.11, 6.7ms\nimage 3/3 /kaggle/working/yolov5/output/test/water/Image2.jpeg: 224x224 stone 0.40, water 0.19, wood 0.10, leather 0.10, fabric 0.06, 7.1ms\nSpeed: 0.3ms pre-process, 6.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp39\u001b[0m\n3 labels saved to runs/predict-cls/exp39/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=./output/test/wood, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 215 layers, 25267786 parameters, 0 gradients, 68.3 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/wood/IMG_1906.JPG: 224x224 wood 0.36, stone 0.32, leather 0.09, fabric 0.07, metal 0.06, 6.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/wood/Image1.jpeg: 224x224 stone 0.48, wood 0.18, leather 0.09, water 0.08, fabric 0.05, 6.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/wood/Image2.jpeg: 224x224 stone 0.41, wood 0.26, leather 0.09, fabric 0.07, metal 0.06, 6.7ms\nSpeed: 0.3ms pre-process, 6.5ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp40\u001b[0m\n3 labels saved to runs/predict-cls/exp40/labels\n","output_type":"stream"}]},{"cell_type":"code","source":"for x in materials:\n    !python classify/predict.py --weights runs/train-cls/exp5/weights/best.pt --source ./output/test/{x}","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:48:37.151579Z","iopub.execute_input":"2022-11-11T06:48:37.152019Z","iopub.status.idle":"2022-11-11T06:50:12.996227Z","shell.execute_reply.started":"2022-11-11T06:48:37.151957Z","shell.execute_reply":"2022-11-11T06:50:12.994942Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/fabric, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/fabric/IMG_1909.JPG: 224x224 leather 0.24, metal 0.15, water 0.12, stone 0.12, paper 0.11, 8.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/fabric/Image1.jpeg: 224x224 metal 0.17, paper 0.17, water 0.12, glass 0.11, fabric 0.11, 8.8ms\nimage 3/3 /kaggle/working/yolov5/output/test/fabric/Image2.jpeg: 224x224 stone 0.28, wood 0.25, leather 0.19, fabric 0.09, metal 0.06, 9.3ms\nSpeed: 0.3ms pre-process, 9.0ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp41\u001b[0m\n3 labels saved to runs/predict-cls/exp41/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/foliage, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/foliage/IMG_1916.JPG: 224x224 foliage 0.43, plastic 0.17, glass 0.12, water 0.07, fabric 0.07, 9.8ms\nimage 2/3 /kaggle/working/yolov5/output/test/foliage/Image1.jpeg: 224x224 foliage 0.55, plastic 0.11, glass 0.11, fabric 0.06, water 0.06, 9.5ms\nimage 3/3 /kaggle/working/yolov5/output/test/foliage/Image2.jpeg: 224x224 foliage 0.48, plastic 0.13, glass 0.12, fabric 0.07, water 0.07, 9.5ms\nSpeed: 0.3ms pre-process, 9.6ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp42\u001b[0m\n3 labels saved to runs/predict-cls/exp42/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/glass, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/glass/IMG_1914.JPG: 224x224 leather 0.22, metal 0.16, paper 0.13, water 0.12, stone 0.09, 8.8ms\nimage 2/3 /kaggle/working/yolov5/output/test/glass/Image1.jpeg: 224x224 leather 0.24, stone 0.20, wood 0.15, metal 0.11, water 0.10, 8.9ms\nimage 3/3 /kaggle/working/yolov5/output/test/glass/Image2.jpeg: 224x224 leather 0.25, metal 0.15, water 0.13, stone 0.12, paper 0.10, 8.9ms\nSpeed: 0.3ms pre-process, 8.9ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp43\u001b[0m\n3 labels saved to runs/predict-cls/exp43/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/leather, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/leather/IMG_1907.JPG: 224x224 stone 0.28, wood 0.23, leather 0.20, fabric 0.09, water 0.07, 9.1ms\nimage 2/3 /kaggle/working/yolov5/output/test/leather/Image1.jpeg: 224x224 leather 0.26, stone 0.16, metal 0.13, water 0.12, wood 0.11, 9.0ms\nimage 3/3 /kaggle/working/yolov5/output/test/leather/Image2.jpeg: 224x224 wood 0.31, stone 0.30, leather 0.15, fabric 0.10, metal 0.05, 9.2ms\nSpeed: 0.3ms pre-process, 9.1ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp44\u001b[0m\n3 labels saved to runs/predict-cls/exp44/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/metal, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/metal/IMG_1912.JPG: 224x224 leather 0.23, metal 0.15, water 0.13, paper 0.12, stone 0.10, 9.0ms\nimage 2/3 /kaggle/working/yolov5/output/test/metal/Image1.jpeg: 224x224 leather 0.19, metal 0.17, paper 0.14, water 0.12, stone 0.10, 8.9ms\nimage 3/3 /kaggle/working/yolov5/output/test/metal/Image2.jpeg: 224x224 leather 0.23, metal 0.15, water 0.13, stone 0.12, paper 0.10, 18.9ms\nSpeed: 0.4ms pre-process, 12.3ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp45\u001b[0m\n3 labels saved to runs/predict-cls/exp45/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/paper, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/paper/IMG_1911.JPG: 224x224 plastic 0.33, water 0.19, paper 0.14, metal 0.11, leather 0.09, 8.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/paper/Image1.jpeg: 224x224 leather 0.18, metal 0.17, paper 0.16, plastic 0.12, water 0.11, 9.2ms\nimage 3/3 /kaggle/working/yolov5/output/test/paper/Image2.jpeg: 224x224 water 0.25, leather 0.16, metal 0.12, stone 0.11, fabric 0.09, 9.2ms\nSpeed: 0.3ms pre-process, 9.1ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp46\u001b[0m\n3 labels saved to runs/predict-cls/exp46/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/plastic, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/plastic/IMG_1913.JPG: 224x224 plastic 0.25, paper 0.19, glass 0.14, metal 0.13, water 0.10, 8.9ms\nimage 2/3 /kaggle/working/yolov5/output/test/plastic/Image1.jpeg: 224x224 water 0.35, fabric 0.19, plastic 0.14, glass 0.12, paper 0.08, 9.2ms\nimage 3/3 /kaggle/working/yolov5/output/test/plastic/Image2.jpeg: 224x224 fabric 0.20, glass 0.17, paper 0.15, metal 0.14, water 0.13, 9.0ms\nSpeed: 0.3ms pre-process, 9.0ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp47\u001b[0m\n3 labels saved to runs/predict-cls/exp47/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/stone, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/stone/IMG_1918.JPG: 224x224 leather 0.19, stone 0.15, metal 0.14, wood 0.12, water 0.10, 8.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/stone/Image1.jpeg: 224x224 leather 0.25, stone 0.18, water 0.14, wood 0.12, metal 0.11, 8.9ms\nimage 3/3 /kaggle/working/yolov5/output/test/stone/Image2.jpeg: 224x224 plastic 0.27, water 0.19, paper 0.15, metal 0.13, leather 0.12, 8.9ms\nSpeed: 0.3ms pre-process, 8.8ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp48\u001b[0m\n3 labels saved to runs/predict-cls/exp48/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/water, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/water/IMG_1915.JPG: 224x224 stone 0.23, leather 0.22, wood 0.18, metal 0.09, water 0.09, 11.5ms\nimage 2/3 /kaggle/working/yolov5/output/test/water/Image1.jpeg: 224x224 plastic 0.29, water 0.20, paper 0.14, metal 0.11, leather 0.10, 12.0ms\nimage 3/3 /kaggle/working/yolov5/output/test/water/Image2.jpeg: 224x224 leather 0.19, metal 0.17, paper 0.16, plastic 0.14, water 0.13, 11.4ms\nSpeed: 0.4ms pre-process, 11.6ms inference, 0.1ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp49\u001b[0m\n3 labels saved to runs/predict-cls/exp49/labels\n\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp5/weights/best.pt'], source=./output/test/wood, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=True, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-237-g55e9516 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nModel summary: 264 layers, 46804410 parameters, 0 gradients, 128.9 GFLOPs\nimage 1/3 /kaggle/working/yolov5/output/test/wood/IMG_1906.JPG: 224x224 wood 0.35, stone 0.30, leather 0.13, fabric 0.10, metal 0.04, 8.8ms\nimage 2/3 /kaggle/working/yolov5/output/test/wood/Image1.jpeg: 224x224 leather 0.25, water 0.17, stone 0.15, metal 0.11, wood 0.10, 8.9ms\nimage 3/3 /kaggle/working/yolov5/output/test/wood/Image2.jpeg: 224x224 stone 0.29, wood 0.26, leather 0.18, fabric 0.09, water 0.06, 8.9ms\nSpeed: 0.3ms pre-process, 8.8ms inference, 0.0ms NMS per image at shape (1, 3, 224, 224)\nResults saved to \u001b[1mruns/predict-cls/exp50\u001b[0m\n3 labels saved to runs/predict-cls/exp50/labels\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Calculate accuracy score for all 5 models (n, s, m, l, x)","metadata":{}},{"cell_type":"code","source":"test_accuracy_exp = 11/30\ntest_accuracy_exp2 = 7/30\ntest_accuracy_exp3 = 9/30\ntest_accuracy_exp4 = 8/30\ntest_accuracy_exp5 = 6/30\n","metadata":{"execution":{"iopub.status.busy":"2022-11-11T06:55:32.682057Z","iopub.execute_input":"2022-11-11T06:55:32.683169Z","iopub.status.idle":"2022-11-11T06:55:32.689679Z","shell.execute_reply.started":"2022-11-11T06:55:32.683125Z","shell.execute_reply":"2022-11-11T06:55:32.688299Z"},"trusted":true},"execution_count":24,"outputs":[]}]}